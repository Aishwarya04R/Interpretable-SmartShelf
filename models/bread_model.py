# -*- coding: utf-8 -*-
"""Untitled26.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tVpN3PXgS8idIrtPKIGhSLSBGbKJmOZ8
"""

from google.colab import drive
drive.mount('/content/drive')

IMAGE_DIR = "/content/drive/MyDrive/bread_dataset1"
CSV_PATH  = "/content/drive/MyDrive/metadata.csv"

import os, pandas as pd

df = pd.read_csv(CSV_PATH)
print(df.head())
print("Unique labels:", df["day"].unique(), "  total samples:", len(df))

import pandas as pd
import numpy as np
import os



# 1. numeric day index (1..12)
df["day_index"] = df["day"].str.extract(r"(\d+)").astype(int)

# 2. max shelf life in your study
max_day = 12

# 3. days remaining (11..0)
df["days_remaining"] = max_day - df["day_index"]

# 4. freshness category from remaining days
def freshness_category(remaining_days):
    if remaining_days >= 5:
        return "Fresh"
    elif remaining_days >= 3:
        return "Mild"
    elif remaining_days >= 1:
        return "Okay"
    else:
        return "Spoiled"


df["freshness"] = df["days_remaining"].apply(freshness_category)

print(df.head())

import tensorflow as tf
from tensorflow.keras import layers, Model

latent_dim = 100

def build_generator(latent_dim=100):
    model = tf.keras.Sequential(name="Generator")
    model.add(layers.Input(shape=(latent_dim,)))
    model.add(layers.Dense(8*8*256, use_bias=False))
    model.add(layers.BatchNormalization())
    model.add(layers.LeakyReLU())
    model.add(layers.Reshape((8, 8, 256)))
    model.add(layers.Conv2DTranspose(128, 5, strides=2, padding='same', use_bias=False))
    model.add(layers.BatchNormalization())
    model.add(layers.LeakyReLU())
    model.add(layers.Conv2DTranspose(64, 5, strides=2, padding='same', use_bias=False))
    model.add(layers.BatchNormalization())
    model.add(layers.LeakyReLU())
    model.add(layers.Conv2DTranspose(3, 5, strides=2, padding='same',
                                 use_bias=False, activation='tanh'))
    return model

def build_discriminator(input_shape=(64,64,3)):
    model = tf.keras.Sequential(name="Discriminator")
    model.add(layers.Input(shape=input_shape))
    model.add(layers.Conv2D(64, 5, strides=2, padding='same'))
    model.add(layers.LeakyReLU(0.2))
    model.add(layers.Dropout(0.3))
    model.add(layers.Conv2D(128, 5, strides=2, padding='same'))
    model.add(layers.LeakyReLU(0.2))
    model.add(layers.Dropout(0.3))
    model.add(layers.Flatten())
    model.add(layers.Dense(1, activation='sigmoid'))
    return model

generator     = build_generator(latent_dim)
discriminator = build_discriminator()

# Compile discriminator first (trainable = True)
discriminator.trainable = True
discriminator.compile(
    optimizer=tf.keras.optimizers.Adam(1e-4),
    loss='binary_crossentropy',
    metrics=['accuracy']
)

# Build combined GAN (freeze discriminator inside GAN)
discriminator.trainable = False
z = layers.Input(shape=(latent_dim,))
fake_img = generator(z)
validity = discriminator(fake_img)
gan = Model(z, validity, name="GAN")
gan.compile(optimizer=tf.keras.optimizers.Adam(1e-4),
            loss='binary_crossentropy')

# Re-enable discriminator for its own training calls
discriminator.trainable = True

from tensorflow.keras.preprocessing.image import load_img, img_to_array

gan_imgs = []
for fname in df["filename"]:
    path = os.path.join(IMAGE_DIR, fname)
    img  = load_img(path, target_size=(64,64))
    arr  = img_to_array(img)
    arr  = (arr / 127.5) - 1.0   # scale to [-1,1] for tanh
    gan_imgs.append(arr)

gan_imgs = np.array(gan_imgs, dtype="float32")
print("GAN training images:", gan_imgs.shape)

epochs = 2000         # you can reduce for quick test (e.g. 500)
batch_size_gan = 32
half_batch = batch_size_gan // 2

for epoch in range(1, epochs+1):
    # ---- Train discriminator ----
    idx = np.random.randint(0, gan_imgs.shape[0], half_batch)
    real = gan_imgs[idx]
    noise = np.random.normal(0, 1, (half_batch, latent_dim))
    fake  = generator.predict(noise, verbose=0)

    real_y = np.ones((half_batch, 1))
    fake_y = np.zeros((half_batch, 1))

    d_loss_real = discriminator.train_on_batch(real, real_y)
    d_loss_fake = discriminator.train_on_batch(fake, fake_y)
    d_loss = 0.5 * (np.array(d_loss_real) + np.array(d_loss_fake))

    # ---- Train generator (via GAN) ----
    noise = np.random.normal(0, 1, (batch_size_gan, latent_dim))
    valid_y = np.ones((batch_size_gan, 1))
    g_loss = gan.train_on_batch(noise, valid_y)

    if epoch % 200 == 0:
        print(f"Epoch {epoch}/{epochs}  D_loss: {d_loss[0]:.4f}  "
              f"D_acc: {d_loss[1]*100:.1f}%  G_loss: {g_loss:.4f}")

generator.save("/content/drive/MyDrive/drigenerator_model.keras")
discriminator.save("/content/drive/MyDrive/discriminator_model.keras")
gan.save("/content/drive/MyDrive/gan_full_model.keras")

from PIL import Image

AUG_DIR = os.path.join(IMAGE_DIR, "gan_aug")
os.makedirs(AUG_DIR, exist_ok=True)

num_synth = 200  # how many GAN images to add

syn_files = []
syn_days  = []

for i in range(num_synth):
    noise = np.random.normal(0, 1, (1, latent_dim))
    gen   = generator.predict(noise, verbose=0)[0]  # 64x64x3, [-1,1]
    gen   = (gen + 1.0) * 127.5
    gen   = np.clip(gen, 0, 255).astype("uint8")

    img_resized = Image.fromarray(gen).resize((224, 224))
    fname = f"gan_{i:04d}.jpg"
    img_resized.save(os.path.join(AUG_DIR, fname))

    syn_files.append(os.path.join("gan_aug", fname))
    # randomly pick a real day label to keep distribution similar
    syn_days.append(np.random.choice(df["day"]))

df_syn = pd.DataFrame({"filename": syn_files, "day": syn_days})
df_syn["day_index"]      = df_syn["day"].str.extract(r"(\d+)").astype(int)
df_syn["days_remaining"] = max_day - df_syn["day_index"]
df_syn["freshness"]      = df_syn["days_remaining"].apply(freshness_category)

# Combined real + GAN metadata
df_all = pd.concat([df, df_syn], ignore_index=True)
print("Real + GAN samples:", len(df_all))

from tensorflow.keras.preprocessing.image import ImageDataGenerator

img_size   = (224, 224)
batch_size = 16

datagen = ImageDataGenerator(
    rescale=1./255,
    validation_split=0.2
)

train_gen = datagen.flow_from_dataframe(
    df_all,
    directory=IMAGE_DIR,
    x_col="filename",
    y_col="days_remaining",    # numeric
    target_size=img_size,
    class_mode="raw",          # regression
    batch_size=batch_size,
    subset="training",
    shuffle=True
)

val_gen = datagen.flow_from_dataframe(
    df_all,
    directory=IMAGE_DIR,
    x_col="filename",
    y_col="days_remaining",
    target_size=img_size,
    class_mode="raw",
    batch_size=batch_size,
    subset="validation",
    shuffle=False
)

print("Train samples:", train_gen.n, " Val samples:", val_gen.n)

from tensorflow.keras import layers, models

def build_msff_regressor(input_shape=(224,224,3)):
    inp = layers.Input(shape=input_shape)
    b1 = layers.Conv2D(32, 3, padding="same", activation="relu")(inp)
    b1 = layers.MaxPooling2D(2)(b1)
    b2 = layers.Conv2D(32, 5, padding="same", activation="relu")(inp)
    b2 = layers.MaxPooling2D(2)(b2)
    b3 = layers.Conv2D(32, 7, padding="same", activation="relu")(inp)
    b3 = layers.MaxPooling2D(2)(b3)
    x = layers.Concatenate()([b1, b2, b3])
    x = layers.Conv2D(64, 3, padding="same", activation="relu")(x)
    x = layers.MaxPooling2D(2)(x)
    x = layers.Conv2D(128, 3, padding="same", activation="relu",
                      name="msff_last_conv")(x)
    x = layers.MaxPooling2D(2)(x)
    x = layers.Flatten()(x)
    x = layers.Dense(256, activation="relu", name="feature_dense")(x)
    x = layers.Dropout(0.5)(x)
    out = layers.Dense(1, activation="linear", name="days_remaining")(x)
    model = models.Model(inp, out, name="MSFF_Regression")
    return model
model = build_msff_regressor(input_shape=(img_size[0], img_size[1], 3))
model.compile(
    optimizer=tf.keras.optimizers.Adam(1e-4),
    loss="mse",
    metrics=["mae"]   # mean absolute error in DAYS
)
model.summary()

import numpy as np
import pandas as pd

max_usable_day = 8   # bread considered usable up to day 8

# If not already present, extract numeric day index
# df_all["day"] should be like "Day 1", "Day 2", ...
df_all["day_index"] = df_all["day"].str.extract(r"(\d+)").astype(int)

# Regression label: remaining shelf life in days
df_all["days_remaining"] = (max_usable_day - df_all["day_index"]).clip(lower=0)

# 4-class mapping based on REMAINING days
def freshness_cat_4(remaining_days):
    if remaining_days >= 5:
        return "Fresh"
    elif remaining_days >= 3:
        return "Mild"
    elif remaining_days >= 1:
        return "Okay"
    else:
        return "Spoiled"

df_all["freshness_4"] = df_all["days_remaining"].apply(freshness_cat_4)

print("4-class distribution in full dataset:")
print(df_all["freshness_4"].value_counts())

from sklearn.model_selection import train_test_split

# Check if any 4-class bucket has fewer than 2 samples
vc4 = df_all["freshness_4"].value_counts()
use_4class_strat = vc4.min() >= 2

if use_4class_strat:
    strat_col = "freshness_4"
    print("\nUsing 4-class stratification on:", strat_col)
else:
    print("\n Not enough samples in one of the 4 classes.")
    print("Falling back to 3-class stratification (Fresh / Mid / Spoiled).")

    # Define 3 classes from remaining days
    def freshness_cat_3(remaining_days):
        if remaining_days >= 4:
            return "Fresh"
        elif remaining_days >= 1:
            return "Mid"
        else:
            return "Spoiled"

    df_all["freshness_3"] = df_all["days_remaining"].apply(freshness_cat_3)
    print("3-class distribution in full dataset:")
    print(df_all["freshness_3"].value_counts())

    strat_col = "freshness_3"

# Finally do the stratified split
train_df, val_df = train_test_split(
    df_all,
    test_size=0.2,
    stratify=df_all[strat_col],
    random_state=42
)

print("\nTrain distribution:")
print(train_df[strat_col].value_counts())
print("\nValidation distribution:")
print(val_df[strat_col].value_counts())

from tensorflow.keras.preprocessing.image import ImageDataGenerator

img_size   = (224, 224)
batch_size = 16

datagen = ImageDataGenerator(rescale=1./255)

train_gen = datagen.flow_from_dataframe(
    train_df,
    directory=IMAGE_DIR,
    x_col="filename",
    y_col="days_remaining",    # regression label
    target_size=img_size,
    class_mode="raw",
    batch_size=batch_size,
    shuffle=True
)

val_gen = datagen.flow_from_dataframe(
    val_df,
    directory=IMAGE_DIR,
    x_col="filename",
    y_col="days_remaining",
    target_size=img_size,
    class_mode="raw",
    batch_size=batch_size,
    shuffle=False
)

print("Train samples:", train_gen.n, "  Val samples:", val_gen.n)

history = model.fit(
    train_gen,
    validation_data=val_gen,
    epochs=50
)

history_tuned = model.fit(
    train_gen,
    validation_data=val_gen,
    epochs=100,
    callbacks=[lr_scheduler, early_stop]
)

val_loss, val_mae = model.evaluate(val_gen, verbose=1)

print(f"Validation MSE  : {val_loss:.4f}")
print(f"Validation MAE  : {val_mae:.4f} days")

import matplotlib.pyplot as plt

plt.figure(figsize=(8,5))
plt.plot(history.history['mae'], label='Train MAE')
plt.plot(history.history['val_mae'], label='Validation MAE')
plt.xlabel("Epochs")
plt.ylabel("MAE (Days)")
plt.title("MAE vs Epochs")
plt.legend()
plt.grid(True)
plt.show()


plt.figure(figsize=(8,5))
plt.plot(history.history['loss'], label='Train Loss (MSE)')
plt.plot(history.history['val_loss'], label='Validation Loss (MSE)')
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.title("Loss vs Epochs")
plt.legend()
plt.grid(True)
plt.show()

import numpy as np

val_gen.reset()
y_true_days = val_df["days_remaining"].values
y_pred_days = model.predict(val_gen).reshape(-1)

print("Pred sample:", y_pred_days[:10])

def freshness_cat_4(remaining_days):
    if remaining_days >= 5:
        return "Fresh"
    elif remaining_days >= 3:
        return "Mild"
    elif remaining_days >= 1:
        return "Okay"
    else:
        return "Spoiled"

y_true_cls = [freshness_cat_4(d) for d in y_true_days]
y_pred_cls = [freshness_cat_4(round(d)) for d in y_pred_days]

lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.5,
    patience=5,
    min_lr=1e-6,
    verbose=1
)

early_stop = tf.keras.callbacks.EarlyStopping(
    monitor='val_loss',
    patience=10,
    restore_best_weights=True,
    verbose=1
)

model.save("/content/drive/MyDrive/msff_bread_quality_model_after_training.keras")