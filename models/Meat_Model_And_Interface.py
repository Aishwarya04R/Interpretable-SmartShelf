# -*- coding: utf-8 -*-
"""MeatAndSeaFinal.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12aP-kWu7eBiktp5yasDXt8YR1E_Lm6c5
"""

from google.colab import drive
drive.mount('/content/drive')

import os
from google.colab import drive

# 1. Mount Drive first to be safe
if not os.path.exists("/content/drive"):
    drive.mount('/content/drive')

# Path to your source folder in Drive
project_root = "/content/drive/MyDrive/Shelf_Life_Project"
folder_name = os.path.join(project_root, "final_src")
requirements_file = "requirements.txt"

requirements_content = """tensorflow>=2.16.0
pandas
numpy
opencv-python
scikit-learn
matplotlib
seaborn
albumentations
"""

def fix_requirements():
    # --- FIX STARTS HERE ---
    # Create the directory if it doesn't exist
    if not os.path.exists(folder_name):
        print(f"Creating missing folder: {folder_name}")
        os.makedirs(folder_name, exist_ok=True)
    # --- FIX ENDS HERE ---

    file_path = os.path.join(folder_name, requirements_file)

    # Overwrite the file with new versions
    with open(file_path, "w") as f:
        f.write(requirements_content)

    print(f"‚úÖ FIXED: Updated {file_path}")
    print("-" * 30)
    print("NEXT STEP: Run the install command below:")
    print(f'!pip install -r "{file_path}"')

if __name__ == "__main__":
    fix_requirements()

!pip install -r "/content/drive/MyDrive/Shelf_Life_Project/final_src/requirements.txt"

import os

# Define the path for the NEW script in 'final_src'
# We use 'final_src' to separate this new attempt from the old 'source' folder
project_root = "/content/drive/MyDrive/Shelf_Life_Project/final_src"
if not os.path.exists(project_root):
    os.makedirs(project_root)

script_path = os.path.join(project_root, "1_curate_data.py")

# The CORRECTED code content
script_content = """
import os
import glob
import pandas as pd
import numpy as np

# ==========================================
# CONFIGURATION
# ==========================================
DATASET_DIR = "/content/drive/MyDrive/Shelf_Life_Project/data"
OUTPUT_CSV = "/content/drive/MyDrive/Shelf_Life_Project/final_src/master_dataset.csv"

# --- THE FIX FOR YOUR MENTOR ---
# Every single category is now standardized to a 5-Day Lifecycle.
# This ensures perfect balance and removes any "bias" towards fish.
LIFECYCLE_MAP = {
    "chicken": 5,
    "beef": 5,
    "mutton": 5,
    "pork": 5,
    "prawn": 5,
    "crab": 5,
    "fish": 5  # <--- FIXED: Now standardized to 5 days like the rest
}

def get_freshness_label(current_day, max_days):
    # Normalized decay ratio (0.0 = Start, 1.0 = End of Life)
    decay_ratio = current_day / max_days

    # 5-Day Logic:
    # Day 1 (0.2) -> Fresh
    # Day 2 (0.4) -> Semi-Fresh
    # Day 3 (0.6) -> Semi-Fresh
    # Day 4 (0.8) -> Spoiled
    # Day 5 (1.0) -> Spoiled

    if decay_ratio <= 0.35:
        return "Fresh"
    elif decay_ratio <= 0.70:
        return "Semi-Fresh"
    else:
        return "Spoiled"

def process_dataset():
    records = []
    print(f"Scanning dataset at: {DATASET_DIR}...")

    if not os.path.exists(DATASET_DIR):
        print(f"‚ùå ERROR: Dataset folder not found at {DATASET_DIR}")
        return

    for category, max_life in LIFECYCLE_MAP.items():
        cat_path = os.path.join(DATASET_DIR, category)

        if not os.path.exists(cat_path):
            print(f"‚ö†Ô∏è Skipping {category} (Folder not found)")
            continue

        # Case-insensitive folder search for 'Day X'
        subfolders = os.listdir(cat_path)
        day_folders = []
        for f in subfolders:
            full_path = os.path.join(cat_path, f)
            if os.path.isdir(full_path) and 'day' in f.lower():
                day_folders.append(full_path)

        for day_folder in day_folders:
            folder_name = os.path.basename(day_folder)
            try:
                # Extract day number
                day_num = int(''.join(filter(str.isdigit, folder_name)))

                # Safety check: If folder says "Day 7" but max is 5, skip or clamp?
                # We skip to ensure data purity for the 5-day model.
                if day_num > max_life:
                    continue

            except ValueError:
                continue

            # Get images
            images = glob.glob(os.path.join(day_folder, "*.*"))
            valid_exts = ['.jpg', '.jpeg', '.png', '.bmp', '.webp']
            images = [x for x in images if os.path.splitext(x)[1].lower() in valid_exts]

            for img_path in images:
                # --- VITAL: NORMALIZATION LOGIC ---
                # We calculate a 'Shelf Life Score' from 1.0 (Fresh) to 0.0 (Spoiled).
                # Since Max Life is uniform (5), this is now perfectly balanced.
                remaining_life_ratio = max(0.0, (max_life - day_num) / max_life)

                status = get_freshness_label(day_num, max_life)

                records.append({
                    "filepath": img_path,
                    "category": category,
                    "freshness_status": status,
                    "shelf_life_score": remaining_life_ratio, # The unified target
                    "actual_day": day_num,
                    "max_days": max_life
                })

    if not records:
        print("‚ùå ERROR: No images found.")
    else:
        df = pd.DataFrame(records)
        df.to_csv(OUTPUT_CSV, index=False)
        print(f"‚úÖ SUCCESS: Metadata created with {len(df)} images.")
        print("-" * 30)
        print("Class Distribution:")
        print(df['category'].value_counts())
        print("-" * 30)
        print(f"Saved to: {OUTPUT_CSV}")

if __name__ == "__main__":
    process_dataset()
"""

# Write the file
with open(script_path, "w") as f:
    f.write(script_content)

print(f"‚úÖ Corrected script created at: {script_path}")
print("   - Fixed Fish lifecycle to 5 days")
print("   - Created 'final_src' folder")
print("Running script now...")
print("="*40)

# Execute
!python "{script_path}"

import pandas as pd
import numpy as np
import os
from sklearn.model_selection import train_test_split

# ==========================================
# CONFIGURATION
# ==========================================
BASE_PATH = "/content/drive/MyDrive/Shelf_Life_Project/final_src"
CSV_PATH = os.path.join(BASE_PATH, "master_dataset.csv")

# Standardized Life Map (All 5 Days now)
# We re-declare this to be safe, though the CSV has the raw data.
MAX_LIFE_DAYS = 5.0

def engineer_data():
    print("üß™ Starting Feature Engineering...")

    if not os.path.exists(CSV_PATH):
        print("‚ùå Error: master_dataset.csv not found. Run Step 1 first.")
        return

    df = pd.read_csv(CSV_PATH)
    print(f"üìä Loaded {len(df)} raw samples.")

    # 1. CLEANING: Ensure we only have valid statuses
    # The 'freshness_status' column was created in Step 1.
    # We double check for any anomalies.
    valid_statuses = ['Fresh', 'Semi-Fresh', 'Spoiled']
    df = df[df['freshness_status'].isin(valid_statuses)]

    # 2. ADVANCED TARGET ENGINEERING
    # We are refining the "Shelf Life Score" (0.0 to 1.0)
    # Step 1 gave us a hard score based on the day (e.g., Day 1 = 0.8).
    # To make the model smarter, we add a tiny bit of "Jitter" (Random noise).
    # Why? Because not every "Day 1" chicken looks identical.
    # Adding jitter (¬±0.05) prevents the model from memorizing exact numbers
    # and forces it to learn the *concept* of decay.

    np.random.seed(42) # Consistent results

    def add_jitter(row):
        score = row['shelf_life_score']
        # Add random noise between -0.05 and +0.05
        jitter = np.random.uniform(-0.05, 0.05)
        new_score = score + jitter
        # Clip to ensure it stays between 0 and 1
        return max(0.0, min(1.0, new_score))

    df['shelf_life_score_target'] = df.apply(add_jitter, axis=1)

    # 3. STRATIFIED SPLITTING
    # We need to split 80% Train, 10% Val, 10% Test.
    # CRITICAL: We must maintain the perfect balance (Fish=Chicken=Beef).
    # We stratify by 'category' to guarantee this.

    # First: Split off 20% for Val/Test
    train_df, temp_df = train_test_split(
        df,
        test_size=0.2,
        stratify=df['category'],
        random_state=42
    )

    # Second: Split that 20% into 10% Val and 10% Test
    val_df, test_df = train_test_split(
        temp_df,
        test_size=0.5,
        stratify=temp_df['category'],
        random_state=42
    )

    # 4. SAVE SPLITS
    train_df.to_csv(os.path.join(BASE_PATH, "train.csv"), index=False)
    val_df.to_csv(os.path.join(BASE_PATH, "val.csv"), index=False)
    test_df.to_csv(os.path.join(BASE_PATH, "test.csv"), index=False)

    print("\n‚úÖ DATASET PREPARED SUCCESSFULLY")
    print("-" * 30)
    print(f"Training Set:   {len(train_df)} images (The Teacher)")
    print(f"Validation Set: {len(val_df)} images (The Practice Exam)")
    print(f"Test Set:       {len(test_df)} images (The Final Exam)")
    print("-" * 30)
    print("Files saved to 'final_src/': train.csv, val.csv, test.csv")

if __name__ == "__main__":
    engineer_data()

import tensorflow as tf
import pandas as pd
import numpy as np
import cv2
import os
from tensorflow.keras.applications import EfficientNetB4
from tensorflow.keras import layers, models, optimizers, callbacks
from sklearn.preprocessing import LabelEncoder

# ==========================================
# CONFIGURATION
# ==========================================
BASE_PATH = "/content/drive/MyDrive/Shelf_Life_Project/final_src"
TRAIN_CSV = os.path.join(BASE_PATH, "train.csv")
VAL_CSV = os.path.join(BASE_PATH, "val.csv")
MODEL_SAVE_PATH = os.path.join(BASE_PATH, "final_smartshelf_model.keras")
BACKUP_DIR = os.path.join(BASE_PATH, "training_backup") # Vital for resuming

# Create backup dir if not exists
if not os.path.exists(BACKUP_DIR):
    os.makedirs(BACKUP_DIR)

# Model Settings
IMG_SIZE = 380
BATCH_SIZE = 16
EPOCHS = 100            # <--- Set to 100 as requested
LEARNING_RATE = 1e-4    # Low LR for stable 100-epoch training

# ==========================================
# 1. DATA GENERATOR
# ==========================================
class MultiTaskDataGenerator(tf.keras.utils.Sequence):
    def __init__(self, df, batch_size=16, img_size=380, shuffle=True, augment=False):
        self.df = df
        self.batch_size = batch_size
        self.img_size = img_size
        self.shuffle = shuffle
        self.augment = augment
        self.indices = np.arange(len(self.df))

        self.le_cat = LabelEncoder()
        self.le_cat.fit(self.df['category'])
        self.le_stat = LabelEncoder()
        self.le_stat.fit(self.df['freshness_status'])

        np.save(os.path.join(BASE_PATH, "classes_category.npy"), self.le_cat.classes_)
        np.save(os.path.join(BASE_PATH, "classes_status.npy"), self.le_stat.classes_)

        self.on_epoch_end()

    def __len__(self):
        return int(np.ceil(len(self.df) / self.batch_size))

    def __getitem__(self, index):
        idx = self.indices[index * self.batch_size:(index + 1) * self.batch_size]
        batch_data = self.df.iloc[idx]

        images, y_cat, y_stat, y_life = [], [], [], []

        for _, row in batch_data.iterrows():
            try:
                img = cv2.imread(row['filepath'])
                if img is None: continue
                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
                img = cv2.resize(img, (self.img_size, self.img_size))

                if self.augment:
                    if np.random.random() > 0.5: img = cv2.flip(img, 1)
                    if np.random.random() > 0.5:
                        rows, cols = img.shape[:2]
                        M = cv2.getRotationMatrix2D((cols/2, rows/2), np.random.randint(-15, 15), 1)
                        img = cv2.warpAffine(img, M, (cols, rows))

                images.append(img)
                y_cat.append(self.le_cat.transform([row['category']])[0])
                y_stat.append(self.le_stat.transform([row['freshness_status']])[0])
                y_life.append(row['shelf_life_score_target'])

            except Exception: continue

        if len(images) == 0: return np.zeros((0, self.img_size, self.img_size, 3)), {}

        X = tf.keras.applications.efficientnet.preprocess_input(np.array(images, dtype=float))

        return X, {
            "output_category": np.array(y_cat),
            "output_status": np.array(y_stat),
            "output_life": np.array(y_life)
        }

    def on_epoch_end(self):
        if self.shuffle: np.random.shuffle(self.indices)

# ==========================================
# 2. MODEL ARCHITECTURE
# ==========================================
def build_model(num_categories, num_statuses):
    inputs = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3))

    # Backbone
    base_model = EfficientNetB4(include_top=False, weights="imagenet", input_tensor=inputs)

    # Fine-Tuning: Unfreeze top 30 layers
    base_model.trainable = True
    for layer in base_model.layers[:-30]:
        layer.trainable = False

    x = layers.GlobalAveragePooling2D()(base_model.output)
    x = layers.BatchNormalization()(x)
    x = layers.Dropout(0.3)(x)

    # Heads
    out_cat = layers.Dense(num_categories, activation="softmax", name="output_category")(x)

    concat_stat = layers.Concatenate()([x, out_cat])
    dense_stat = layers.Dense(64, activation="relu")(concat_stat)
    out_stat = layers.Dense(num_statuses, activation="softmax", name="output_status")(dense_stat)

    concat_life = layers.Concatenate()([x, out_cat, out_stat])
    dense_life = layers.Dense(32, activation="relu")(concat_life)
    out_life = layers.Dense(1, activation="sigmoid", name="output_life")(dense_life)

    model = models.Model(inputs=inputs, outputs=[out_cat, out_stat, out_life])
    return model

# ==========================================
# 3. MAIN EXECUTION
# ==========================================
if __name__ == "__main__":
    print("üöÄ Initializing 100-Epoch Training Pipeline...")

    train_df = pd.read_csv(TRAIN_CSV)
    val_df = pd.read_csv(VAL_CSV)

    train_gen = MultiTaskDataGenerator(train_df, BATCH_SIZE, IMG_SIZE, shuffle=True, augment=True)
    val_gen = MultiTaskDataGenerator(val_df, BATCH_SIZE, IMG_SIZE, shuffle=False, augment=False)

    num_cats = len(train_gen.le_cat.classes_)
    num_stats = len(train_gen.le_stat.classes_)

    model = build_model(num_cats, num_stats)

    model.compile(
        optimizer=optimizers.Adam(learning_rate=LEARNING_RATE),
        loss={
            "output_category": "sparse_categorical_crossentropy",
            "output_status": "sparse_categorical_crossentropy",
            "output_life": "mean_absolute_error"
        },
        loss_weights={"output_category": 1.0, "output_status": 1.0, "output_life": 2.5},
        metrics={"output_category": "accuracy", "output_status": "accuracy"}
    )

    # --- PRO CALLBACKS ---
    # 1. BackupAndRestore: Allows pausing and resuming training!
    backup_cb = callbacks.BackupAndRestore(backup_dir=BACKUP_DIR)

    # 2. ModelCheckpoint: Saves ONLY the best version
    checkpoint_cb = callbacks.ModelCheckpoint(MODEL_SAVE_PATH, save_best_only=True, monitor="val_loss", verbose=1)

    # 3. EarlyStopping: "Ma'am, it reached peak accuracy early!"
    early_stop_cb = callbacks.EarlyStopping(monitor="val_loss", patience=10, restore_best_weights=True)

    print(">>> Starting Training...")
    print(">>> Note: If Colab disconnects, just re-run this script. It will resume automatically.")

    history = model.fit(
        train_gen,
        epochs=EPOCHS,
        validation_data=val_gen,
        callbacks=[backup_cb, checkpoint_cb, early_stop_cb]
    )

    print("‚úÖ Training Complete. Best model saved.")

import tensorflow as tf
import pandas as pd
import numpy as np
import cv2
import os
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report, confusion_matrix, mean_absolute_error, r2_score

# ==========================================
# CONFIGURATION
# ==========================================
BASE_DIR = "/content/drive/MyDrive/Shelf_Life_Project/final_src"
CSV_FILE = os.path.join(BASE_DIR, "master_dataset.csv")
MODEL_PATH = os.path.join(BASE_DIR, "final_smartshelf_model.keras")

# --- THE FIX: MATCH MODEL INPUT SIZE ---
IMG_SIZE = 380  # Changed from 224 to 380 to match your trained model
BATCH_SIZE = 16
TEST_SAMPLE_SIZE = 300

# Lifecycle Map (All 5 Days as per new standard)
LIFECYCLE_MAP = {
    'chicken': 5, 'beef': 5, 'mutton': 5, 'pork': 5,
    'prawn': 5, 'crab': 5, 'fish': 5
}

# ==========================================
# 1. PREPARE DATA
# ==========================================
print(">>> Loading Data...")
if not os.path.exists(CSV_FILE):
    raise FileNotFoundError(f"CSV not found at {CSV_FILE}")

df = pd.read_csv(CSV_FILE)

le_cat = LabelEncoder()
df['cat_encoded'] = le_cat.fit_transform(df['category'])
le_stat = LabelEncoder()
df['status_encoded'] = le_stat.fit_transform(df['freshness_status'])

# Get Test Split (Same random_state as training to prevent leakage)
train_df, temp_df = train_test_split(df, test_size=0.2, stratify=df['category'], random_state=42)
val_df, test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df['category'], random_state=42)

# Subsample for speed
if len(test_df) > TEST_SAMPLE_SIZE:
    print(f">>> Selecting {TEST_SAMPLE_SIZE} random images for rapid evaluation...")
    test_df = test_df.sample(n=TEST_SAMPLE_SIZE, random_state=42)

# ==========================================
# 2. PREDICTIONS
# ==========================================
print(">>> Loading Model...")
if not os.path.exists(MODEL_PATH):
    raise FileNotFoundError(f"Model not found at {MODEL_PATH}")

model = tf.keras.models.load_model(MODEL_PATH)

print(">>> Running Predictions...")
actual_cats, pred_cats = [], []
actual_stats, pred_stats = [], []
actual_life, pred_life = [], []

for i in range(0, len(test_df), BATCH_SIZE):
    batch = test_df.iloc[i:i+BATCH_SIZE]
    images = []
    for _, row in batch.iterrows():
        try:
            img = cv2.imread(row['filepath'])
            if img is None: continue
            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
            img = cv2.resize(img, (IMG_SIZE, IMG_SIZE)) # Resizing to 380x380 here
            images.append(img)

            actual_cats.append(row['cat_encoded'])
            actual_stats.append(row['status_encoded'])
            actual_life.append(row['shelf_life_score'])
        except: continue

    if not images: continue

    # Predict
    img_batch = tf.keras.applications.efficientnet.preprocess_input(np.array(images, dtype=float))
    preds = model.predict(img_batch, verbose=0)

    pred_cats.extend(np.argmax(preds[0], axis=1))
    pred_stats.extend(np.argmax(preds[1], axis=1))
    pred_life.extend(preds[2].flatten())

    print(f"   Processed {min(i+BATCH_SIZE, len(test_df))}/{len(test_df)}", end='\r')

print("\n>>> Calculation Complete!")

# ==========================================
# 3. GENERATE VISUALS
# ==========================================
y_cat_true, y_cat_pred = np.array(actual_cats), np.array(pred_cats)
y_stat_true, y_stat_pred = np.array(actual_stats), np.array(pred_stats)
y_life_true, y_life_pred = np.array(actual_life), np.array(pred_life)

# --- Confusion Matrix Plot ---
fig, axes = plt.subplots(1, 2, figsize=(20, 8))

cm_food = confusion_matrix(y_cat_true, y_cat_pred)
sns.heatmap(cm_food, annot=True, fmt='d', cmap='Blues', xticklabels=le_cat.classes_, yticklabels=le_cat.classes_, ax=axes[0])
axes[0].set_title('Confusion Matrix: Food Type')

cm_stat = confusion_matrix(y_stat_true, y_stat_pred)
sns.heatmap(cm_stat, annot=True, fmt='d', cmap='Greens', xticklabels=le_stat.classes_, yticklabels=le_stat.classes_, ax=axes[1])
axes[1].set_title('Confusion Matrix: Freshness')

plt.tight_layout()
save_path_cm = os.path.join(BASE_DIR, "evaluation_confusion_matrices.png")
plt.savefig(save_path_cm)
print(f"‚úÖ Saved Matrix: {save_path_cm}")
plt.show()

# --- Regression Plot ---
inv_cat_map = {i: label for i, label in enumerate(le_cat.classes_)}
max_lives = np.array([LIFECYCLE_MAP[inv_cat_map[i]] for i in y_cat_true])

days_true = y_life_true * max_lives
days_pred = y_life_pred * max_lives
mae = mean_absolute_error(days_true, days_pred)

plt.figure(figsize=(10, 6))
plt.scatter(days_true, days_pred, alpha=0.6, color='#667eea')
# Plot line from 0 to 5 (Since max life is now 5 days)
plt.plot([0, 5], [0, 5], 'r--', lw=2)
plt.xlabel('Actual Days')
plt.ylabel('Predicted Days')
plt.title(f'Shelf Life Regression (MAE: {mae:.2f} Days)')
plt.grid(True, alpha=0.3)

save_path_reg = os.path.join(BASE_DIR, "evaluation_regression_plot.png")
plt.savefig(save_path_reg)
print(f"‚úÖ Saved Regression Plot: {save_path_reg}")
plt.show()

# --- TEXT REPORT ---
print("\n" + "="*60)
print("üìä FINAL METRICS REPORT")
print("="*60)
print("\n--- FOOD CLASSIFICATION ---")
print(classification_report(y_cat_true, y_cat_pred, target_names=le_cat.classes_))
print("\n--- SHELF LIFE REGRESSION ---")
print(f"Mean Absolute Error: {mae:.4f} Days")
print(f"R2 Score:           {r2_score(y_life_true, y_life_pred):.4f}")

import os
import numpy as np
import tensorflow as tf
import cv2
import matplotlib.pyplot as plt

# ==========================================
# CONFIGURATION
# ==========================================
# 1. Path to your saved model (from the training you just did)
MODEL_PATH = "/content/drive/MyDrive/Shelf_Life_Project/final_src/final_smartshelf_model.keras"

# 2. Path to an image you want to test (CHANGE THIS to a real image path)
# Example: Pick a random chicken or fish image from your data folder to test
TEST_IMAGE_PATH = "/content/drive/MyDrive/Shelf_Life_Project/data/crab/day 1/MBCE_Crab_Meat - Copy (10) - Copy.jpg"

# 3. Domain Knowledge (Max Shelf Life)
# Updated to reflect the standardized 5-day cycle for all categories
LIFECYCLE_MAP = {
    "chicken": 5, "beef": 5, "mutton": 5, "pork": 5,
    "prawn": 5, "crab": 5, "fish": 5
}

def test_prediction(image_path):
    if not os.path.exists(MODEL_PATH):
        print("‚ùå Error: Model file not found. Did training save correctly?")
        return

    print(f"Loading AI Model from: {MODEL_PATH}...")
    model = tf.keras.models.load_model(MODEL_PATH)

    # Load the Encoders (to translate 0,1,2 back to "Chicken", "Fresh", etc.)
    try:
        cat_classes = np.load("/content/drive/MyDrive/Shelf_Life_Project/final_src/classes_category.npy", allow_pickle=True)
        stat_classes = np.load("/content/drive/MyDrive/Shelf_Life_Project/final_src/classes_status.npy", allow_pickle=True)
    except:
        print("‚ùå Error: Encoder files (.npy) not found in final_src folder.")
        return

    # Preprocess the Image
    if not os.path.exists(image_path):
        print(f"‚ùå Error: Image not found at {image_path}")
        return

    img = cv2.imread(image_path)
    rgb_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

    # Resize to 380x380 (High Accuracy Model Size)
    # If you used the Ultra-Lite script, change this to 224
    resized_img = cv2.resize(rgb_img, (380, 380))
    input_data = np.expand_dims(resized_img, axis=0) # Shape becomes (1, 380, 380, 3)

    # Preprocess for EfficientNet (same as training)
    input_data = tf.keras.applications.efficientnet.preprocess_input(input_data.astype(np.float32))

    # === MAKE PREDICTION ===
    print("Analyzing image...")
    preds = model.predict(input_data, verbose=0)

    # 1. Decode Food Category
    pred_cat_idx = np.argmax(preds[0])
    food_type = cat_classes[pred_cat_idx]
    confidence = np.max(preds[0])

    # 2. Decode Freshness Status
    pred_stat_idx = np.argmax(preds[1])
    status = stat_classes[pred_stat_idx]

    # 3. Decode Shelf Life (Regression Score 0.0 - 1.0)
    life_score = float(preds[2][0][0])

    # === CALCULATE DAYS ===
    max_life = LIFECYCLE_MAP.get(food_type, 5)

    # Logic: Score 1.0 = 100% Life Left. Score 0.0 = 0% Life Left.
    days_remaining = life_score * max_life

    # Safety Clamp (Can't have negative days or more than max)
    days_remaining = max(0.0, min(max_life, days_remaining))

    # === DISPLAY RESULTS ===
    plt.figure(figsize=(5,5))
    plt.imshow(rgb_img)
    plt.axis('off')
    plt.title(f"{food_type.upper()} | {status}")
    plt.show()

    print("\n" + "="*40)
    print(f"ü•© DETECTED FOOD:   {food_type.upper()} (Confidence: {confidence*100:.1f}%)")
    print(f"üîç VISUAL STATUS:   {status}")
    print("-" * 40)
    print(f"üìâ FRESHNESS SCORE: {life_score:.4f} (0=Spoiled, 1=Fresh)")
    print(f"‚è≥ EST. SHELF LIFE: {days_remaining:.1f} Days")
    print(f"üìÖ MAX LIFECYCLE:   {max_life} Days")
    print("="*40)

    # Smart Logic Check
    if status == "Spoiled" and days_remaining > 1.0:
        print("‚ö†Ô∏è AI WARNING: Visuals show spoilage, but regression is high. TRUST VISUALS -> DISCARD.")
    elif status == "Fresh" and days_remaining < 1.0:
        print("‚ö†Ô∏è AI WARNING: Visuals look fresh, but predicted life is low. CONSUME IMMEDIATELY.")

# === RUN THE TEST ===
# IMPORTANT: Update 'TEST_IMAGE_PATH' at the top to a real image in your drive!
test_prediction(TEST_IMAGE_PATH)

import tensorflow as tf
import pandas as pd
import numpy as np
import cv2
import os
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.applications import EfficientNetB4
from tensorflow.keras import layers, models, optimizers

# ==========================================
# CONFIGURATION
# ==========================================
CSV_FILE = "/content/drive/MyDrive/Shelf_Life_Project/final_src/master_dataset.csv"
IMG_SIZE = 380
EPOCHS_PER_EXP = 3  # Optimized for speed (enough to show the trend)

# The 3 Configurations to Test
EXPERIMENTS = [
    {"id": "Exp_A", "lr": 1e-3, "batch": 16, "dropout": 0.3, "desc": "High LR (0.001)"},
    {"id": "Exp_B", "lr": 1e-4, "batch": 16, "dropout": 0.5, "desc": "Target LR (0.0001)"},
    {"id": "Exp_C", "lr": 1e-4, "batch": 32, "dropout": 0.4, "desc": "High Batch (32)"}
]

# ==========================================
# DATA GENERATOR
# ==========================================
class MultiTaskDataGenerator(tf.keras.utils.Sequence):
    def __init__(self, df, batch_size=16, img_size=380, shuffle=True):
        self.df = df
        self.batch_size = batch_size
        self.img_size = img_size
        self.shuffle = shuffle
        self.indices = np.arange(len(self.df))

        # Initialize encoders
        self.le_cat = LabelEncoder()
        self.le_stat = LabelEncoder()

        # Fit encoders on the dataframe columns
        self.le_cat.fit(self.df['category'])
        self.le_stat.fit(self.df['freshness_status'])

        self.on_epoch_end()

    def __len__(self):
        return int(np.ceil(len(self.df) / self.batch_size))

    def __getitem__(self, index):
        idx = self.indices[index * self.batch_size:(index + 1) * self.batch_size]
        batch_data = self.df.iloc[idx]
        images, y_cat, y_stat, y_life = [], [], [], []
        for _, row in batch_data.iterrows():
            img = cv2.imread(row['filepath'])
            if img is None: continue
            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
            img = cv2.resize(img, (self.img_size, self.img_size))
            images.append(img)

            # Use encoders to transform labels
            y_cat.append(self.le_cat.transform([row['category']])[0])
            y_stat.append(self.le_stat.transform([row['freshness_status']])[0])
            y_life.append(row['shelf_life_score'])

        # Standard EfficientNet Preprocessing
        if len(images) > 0:
             images = tf.keras.applications.efficientnet.preprocess_input(np.array(images, dtype=float))

        return np.array(images), {"output_category": np.array(y_cat), "output_status": np.array(y_stat), "output_life": np.array(y_life)}

    def on_epoch_end(self):
        if self.shuffle: np.random.shuffle(self.indices)

# ==========================================
# MODEL BUILDER
# ==========================================
def build_model(num_categories, num_statuses, dropout_rate):
    inputs = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3))

    # Augmentation
    x = layers.RandomFlip("horizontal_and_vertical")(inputs)
    x = layers.RandomRotation(0.2)(x)

    # Backbone
    base_model = EfficientNetB4(include_top=False, weights="imagenet", input_tensor=x)
    base_model.trainable = False # Faster training for tuning

    x = layers.GlobalAveragePooling2D()(base_model.output)
    x = layers.BatchNormalization()(x)
    x = layers.Dropout(dropout_rate)(x)

    # Heads
    dense_cat = layers.Dense(256, activation="swish")(x)
    out_cat = layers.Dense(num_categories, activation="softmax", name="output_category")(dense_cat)

    concat_stat = layers.Concatenate()([x, dense_cat])
    dense_stat = layers.Dense(128, activation="swish")(concat_stat)
    out_stat = layers.Dense(num_statuses, activation="softmax", name="output_status")(dense_stat)

    concat_life = layers.Concatenate()([x, dense_stat])
    dense_life = layers.Dense(64, activation="swish")(concat_life)
    out_life = layers.Dense(1, activation="sigmoid", name="output_life")(dense_life)

    model = models.Model(inputs=inputs, outputs=[out_cat, out_stat, out_life])
    return model

# ==========================================
# EXECUTION
# ==========================================
if __name__ == "__main__":
    print(">>> Loading Dataset...")
    df = pd.read_csv(CSV_FILE)

    # NOTE: Encoders are now handled inside the DataGenerator to ensure consistency during batching

    # Stratified Split
    train_df, val_df = train_test_split(df, test_size=0.15, stratify=df['category'], random_state=42)

    results_table = []

    print(f">>> Starting Analysis ({len(EXPERIMENTS)} Experiments)...")
    print("="*60)

    for exp in EXPERIMENTS:
        print(f"\nüß™ Experiment: {exp['id']} ({exp['desc']})")

        train_gen = MultiTaskDataGenerator(train_df, exp['batch'], IMG_SIZE)
        val_gen = MultiTaskDataGenerator(val_df, exp['batch'], IMG_SIZE)

        # Get number of classes from the generator's internal encoders
        num_cats = len(train_gen.le_cat.classes_)
        num_stats = len(train_gen.le_stat.classes_)

        model = build_model(num_cats, num_stats, exp['dropout'])

        model.compile(
            optimizer=optimizers.Adam(learning_rate=exp['lr']),
            loss={"output_category": "sparse_categorical_crossentropy", "output_status": "sparse_categorical_crossentropy", "output_life": "mean_absolute_error"},
            loss_weights={"output_category": 1.0, "output_status": 1.0, "output_life": 2.5},
            metrics={"output_category": "accuracy", "output_status": "accuracy"}
        )

        history = model.fit(train_gen, epochs=EPOCHS_PER_EXP, validation_data=val_gen, verbose=1)

        # Capture Metrics
        acc_food = history.history['val_output_category_accuracy'][-1]
        acc_fresh = history.history['val_output_status_accuracy'][-1]
        loss_val = history.history['val_loss'][-1]

        results_table.append({
            "Experiment": exp['id'],
            "Parameters": f"LR={exp['lr']}, Batch={exp['batch']}",
            "Food Accuracy": f"{acc_food:.2%}",
            "Freshness Accuracy": f"{acc_fresh:.2%}",
            "Final Loss": f"{loss_val:.4f}"
        })

        tf.keras.backend.clear_session()
        del model

    # ==========================================
    # SHOW RESULTS
    # ==========================================
    results_df = pd.DataFrame(results_table)

    print("\n" + "="*60)
    print("‚úÖ FINAL HYPERPARAMETER ANALYSIS TABLE")
    print("="*60)
    print(results_df.to_markdown(index=False))

    results_df.to_csv("/content/drive/MyDrive/Shelf_Life_Project/final_src/hyperparameter_results.csv", index=False)
    print("\nSaved to: final_src/hyperparameter_results.csv")

# Commented out IPython magic to ensure Python compatibility.
# %%writefile /content/drive/MyDrive/Shelf_Life_Project/final_src/xai_utils.py
# import numpy as np
# import cv2
# from lime import lime_image
# from skimage.segmentation import mark_boundaries
# 
# # Try importing backends safely
# try:
#     import tensorflow as tf
# except ImportError:
#     tf = None
# 
# try:
#     import torch
#     import torch.nn.functional as F
# except ImportError:
#     torch = None
# 
# class ExplainableAI:
#     """
#     Universal HD XAI: Generates high-resolution Grad-CAM and LIME visualizations
#     for TensorFlow (Meat/Fruit/Bakery) and PyTorch (Veg) models.
#     Updated to ensure Bakery heatmap works and outputs match input size.
#     """
# 
#     def __init__(self, model, classes, backend="tensorflow"):
#         self.model = model
#         self.classes = classes
#         self.backend = backend # 'tensorflow' or 'pytorch'
# 
#     # ==========================
#     # TENSORFLOW LOGIC
#     # ==========================
#     def _get_tf_last_conv(self):
#         """Finds the best convolutional layer for high-detail gradients"""
#         # 1. Specific Bakery Layer (CRITICAL CHECK)
#         try:
#             if self.model.get_layer('msff_last_conv'): return 'msff_last_conv'
#         except: pass
# 
#         # 2. EfficientNet Naming (Meat)
#         for layer in reversed(self.model.layers):
#             if 'top_activation' in layer.name: return layer.name
# 
#         # 3. Fallback: Find last generic Conv2D
#         for layer in reversed(self.model.layers):
#             if isinstance(layer, tf.keras.layers.Conv2D): return layer.name
#             if 'conv' in layer.name.lower(): return layer.name
# 
#         return None
# 
#     def _grad_cam_tf(self, image_array):
#         layer_name = self._get_tf_last_conv()
#         if not layer_name: return None, None
# 
#         # Robustly handle input dimensions
#         if image_array.ndim == 3: input_tensor = np.expand_dims(image_array, 0)
#         else: input_tensor = image_array
# 
#         is_multi_head = isinstance(self.model.output, list) and len(self.model.output) > 1
# 
#         # Build Gradient Model
#         if is_multi_head:
#             grad_model = tf.keras.models.Model(
#                 inputs=self.model.inputs,
#                 outputs=[self.model.get_layer(layer_name).output, self.model.output[0], self.model.output[1]]
#             )
#         else:
#             grad_model = tf.keras.models.Model(
#                 inputs=self.model.inputs,
#                 outputs=[self.model.get_layer(layer_name).output, self.model.output]
#             )
# 
#         with tf.GradientTape(persistent=True) as tape:
#             outputs = grad_model(input_tensor)
#             conv_out = outputs[0]
# 
#             if is_multi_head:
#                 pred_id, pred_status = outputs[1], outputs[2]
#                 loss_id = pred_id[0][tf.argmax(pred_id[0])]
#                 loss_status = pred_status[0][tf.argmax(pred_status[0])]
#             else:
#                 pred_status = outputs[1]
#                 # For Bakery (Regression), we use the output value itself
#                 # CRITICAL FIX: Ensure we get a scalar loss for gradients
#                 if pred_status.shape[-1] == 1:
#                     loss_status = pred_status[0][0]
#                 else:
#                     loss_status = pred_status[0][tf.argmax(pred_status[0])]
#                 loss_id = None
# 
#         # Generate Gradients
#         grads_st = tape.gradient(loss_status, conv_out)
# 
#         # FIX FOR BAKERY: Use ABSOLUTE gradients for regression.
#         # This highlights features that strongly affect the outcome (positively OR negatively).
#         # This ensures Mold (negative impact) shows up as Red.
#         if not is_multi_head and pred_status.shape[-1] == 1:
#              pool_st = tf.reduce_mean(tf.abs(grads_st), axis=(0, 1, 2))
#         else:
#              pool_st = tf.reduce_mean(grads_st, axis=(0, 1, 2))
# 
#         # Weighted Combination (Status/Quality Heatmap)
#         hm_st = tf.maximum(conv_out[0] @ pool_st[..., tf.newaxis], 0)
# 
#         # Robust Normalization
#         hm_st = hm_st.numpy().squeeze()
#         if np.max(hm_st) != 0:
#             hm_st = (hm_st - np.min(hm_st)) / (np.max(hm_st) - np.min(hm_st) + 1e-10)
# 
#         # Identity Heatmap Logic
#         hm_id = None
#         if is_multi_head:
#             grads_id = tape.gradient(loss_id, conv_out)
#             pool_id = tf.reduce_mean(grads_id, axis=(0, 1, 2))
#             hm_id = tf.maximum(conv_out[0] @ pool_id[..., tf.newaxis], 0)
#             hm_id = hm_id.numpy().squeeze()
#             if np.max(hm_id) != 0:
#                 hm_id = (hm_id - np.min(hm_id)) / (np.max(hm_id) - np.min(hm_id) + 1e-10)
#         else:
#             # For single head (Bakery), duplicate the valid heatmap so both slots work
#             hm_id = hm_st
# 
#         del tape
#         return hm_id, hm_st
# 
#     # ==========================
#     # PYTORCH LOGIC (VEG)
#     # ==========================
#     def _grad_cam_pytorch(self, image_tensor):
#         target_layer = None
#         for name, module in self.model.named_modules():
#             if isinstance(module, torch.nn.Conv2d): target_layer = module
#         if target_layer is None: return None, None
# 
#         gradients, activations = [], []
#         def b_hook(m, i, o): gradients.append(o[0])
#         def f_hook(m, i, o): activations.append(o)
# 
#         h1 = target_layer.register_forward_hook(f_hook)
#         h2 = target_layer.register_full_backward_hook(b_hook)
# 
#         self.model.eval()
#         preds = self.model(image_tensor)
# 
#         # Target Freshness Head (Index 1) for Multi-Task Veg
#         if isinstance(preds, tuple): score = preds[1].max()
#         else: score = preds.max()
# 
#         self.model.zero_grad()
#         score.backward()
#         h1.remove(); h2.remove()
# 
# 
# 
#         grads = gradients[0].cpu().data.numpy()[0]
#         fmap = activations[0].cpu().data.numpy()[0]
#         weights = np.mean(grads, axis=(1, 2))
# 
#         heatmap = np.zeros(fmap.shape[1:], dtype=np.float32)
#         for i, w in enumerate(weights):
#             heatmap += w * fmap[i, :, :]
# 
#         heatmap = np.maximum(heatmap, 0)
#         heatmap /= (np.max(heatmap) + 1e-10)
# 
#         # Return single heatmap for PyTorch currently
#         return None, heatmap
# 
# 
#     # ==========================
#     # PUBLIC METHODS
#     # ==========================
#     def grad_cam(self, image_input):
#         if self.backend == "tensorflow":
#             return self._grad_cam_tf(image_input)
#         elif self.backend == "pytorch":
#             # Convert HWC numpy to NCHW tensor for PyTorch
#             tensor = torch.from_numpy(image_input).permute(2, 0, 1).unsqueeze(0).float()
#             return self._grad_cam_pytorch(tensor)
# 
#     def visualize_grad_cam(self, image_array, heatmap, alpha=0.5):
#         if heatmap is None: return image_array
# 
#         if image_array.ndim == 4: image_array = image_array[0]
# 
#         # Ensure image is 0-255 uint8
#         if image_array.max() <= 1.0: img_u8 = (image_array * 255).astype(np.uint8)
#         else: img_u8 = image_array.astype(np.uint8)
# 
#         # HD Resize (Cubic)
#         heatmap = cv2.resize(heatmap, (img_u8.shape[1], img_u8.shape[0]), interpolation=cv2.INTER_CUBIC)
# 
#         heatmap = np.uint8(255 * heatmap)
#         heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)
#         heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)
# 
#         # Blend
#         return cv2.addWeighted(img_u8, 1 - alpha, heatmap, alpha, 0)
# 
#     def lime_explanation(self, image_array, target_head=0, num_samples=30):
#         explainer = lime_image.LimeImageExplainer()
# 
#         if image_array.ndim == 4: image_array = image_array[0]
# 
#         # TF Wrapper
#         def predict_fn_tf(images):
#             # Preprocess
#             processed = tf.keras.applications.efficientnet.preprocess_input(images.astype(np.float32))
#             preds = self.model.predict(processed, verbose=0)
#             is_multi = isinstance(self.model.output, list) and len(self.model.output) > 1
#             if is_multi: return preds[target_head]
#             return preds
# 
#         # PyTorch Wrapper
#         def predict_fn_torch(images):
#             imgs_float = images.astype(np.float32) / 255.0
#             mean, std = np.array([0.485, 0.456, 0.406]), np.array([0.229, 0.224, 0.225])
#             imgs_norm = (imgs_float - mean) / std
#             tensor = torch.from_numpy(imgs_norm).permute(0, 3, 1, 2).float()
#             self.model.eval()
#             with torch.no_grad():
#                 preds = self.model(tensor)
#                 if isinstance(preds, tuple): return preds[1].numpy()
#                 return preds.numpy()
# 
#         predict_fn = predict_fn_torch if self.backend == 'pytorch' else predict_fn_tf
#         img_lime = image_array.astype(np.double)
#         if img_lime.max() <= 1.0: img_lime *= 255.0
# 
#         # LIME for Regression/Single Class
#         explanation = explainer.explain_instance(
#             img_lime, predict_fn, top_labels=1, hide_color=0, num_samples=num_samples
#         )
#         return explanation
# 
#     def visualize_lime(self, explanation, label_idx, num_features=5):
#         try:
#             # Check if regression (no top_labels usually, or label is 0)
#             # LIME for regression returns intercept/local_pred etc.
#             # explain_instance returns an Explanation object.
#             # For regression, we typically visualize the only available label (usually 0 if not specified, or we check top_labels)
# 
#             # Safe Access
#             if explanation.mode == 'regression':
#                  label = list(explanation.local_exp.keys())[0] # Get the only label key
#             else:
#                  label = label_idx if label_idx in explanation.local_exp else explanation.top_labels[0]
# 
#             temp, mask = explanation.get_image_and_mask(label, positive_only=True, num_features=num_features, hide_rest=False)
#         except Exception as e:
#             # Fallback
#             print(f"LIME Viz Error: {e}. Using default label.")
#             temp, mask = explanation.get_image_and_mask(explanation.top_labels[0], positive_only=True, num_features=num_features, hide_rest=False)
# 
#         return mark_boundaries(temp / 255.0, mask, color=(1, 1, 0), mode='thick')
# 
#     def analyze_heatmap_focus(self, heatmap):
#         if heatmap is None: return "General Area"
#         h, w = heatmap.shape
#         regions = {"Top": np.mean(heatmap[0:h//3, :]), "Center": np.mean(heatmap[h//3:2*h//3, :]), "Bottom": np.mean(heatmap[2*h//3:, :])}
#         return max(regions, key=regions.get)

# Commented out IPython magic to ensure Python compatibility.
# %%writefile /content/drive/MyDrive/Shelf_Life_Project/final_src/novelty_features.py
# import streamlit as st
# import numpy as np
# import plotly.graph_objects as go
# from datetime import datetime, timedelta
# import pandas as pd
# 
# class ConfidenceExplainer:
#     @staticmethod
#     def explain_confidence(confidence, food_type):
#         if confidence > 0.95:
#             return f"üéØ **Extremely Confident**: The AI is {confidence*100:.1f}% certain this is {food_type}.", "Very High", "#06d6a0"
#         elif confidence > 0.85:
#             return f"‚úÖ **Highly Confident**: The AI is {confidence*100:.1f}% sure this is {food_type}.", "High", "#26de81"
#         elif confidence > 0.70:
#             return f"‚ö†Ô∏è **Moderately Confident**: The AI is {confidence*100:.1f}% confident.", "Moderate", "#fed330"
#         else:
#             return f"‚ùì **Low Confidence**: The AI is unsure. Check lighting.", "Low", "#ef476f"
# 
# class QualityDecayPredictor:
#     @staticmethod
#     def predict_decay_curve(current_score, shelf_life_days):
#         # Handle 0 days case to prevent math errors
#         if shelf_life_days < 0.1: shelf_life_days = 0.5
# 
#         hours = np.linspace(0, shelf_life_days * 24, 100)
#         days = hours / 24
#         decay_rate = -np.log(0.1) / (shelf_life_days * 24)
#         quality_scores = current_score * 100 * np.exp(-decay_rate * hours)
# 
#         fig = go.Figure()
#         fig.add_trace(go.Scatter(x=days, y=quality_scores, mode='lines', name='Quality', line=dict(color='#667eea', width=3), fill='tozeroy'))
#         fig.add_hline(y=50, line_dash="dash", line_color="orange", annotation_text="Safety Threshold")
#         fig.add_hline(y=20, line_dash="dash", line_color="red", annotation_text="Spoiled")
# 
#         fig.update_layout(title="<b>Predicted Quality Decay</b>", xaxis_title="Days", yaxis_title="Score", yaxis_range=[0, 105], height=350, margin=dict(l=20, r=20, t=40, b=20))
#         return fig
# 
# class SmartStorageAdvisor:
#     @staticmethod
#     def generate_recommendations(food_type, score, days):
#         recs = []
#         if score < 40:
#             recs.append(("‚ùå", "Do Not Consume", "Food appears spoiled."))
#         elif days < 1.0:
#             recs.append(("üç≥", "Cook Immediately", "Shelf life is critical."))
#         else:
#             recs.append(("‚ùÑÔ∏è", "Refrigerate", f"Keep {food_type} at 0-4¬∞C."))
#             if days > 2:
#                 recs.append(("üßä", "Freeze", "Freeze to extend life by months."))
#         return recs
# 
# class CarbonFootprintCalculator:
#     CO2_MAP = {'fish': 5.5, 'chicken': 6.9, 'beef': 27.0, 'pork': 12.1, 'mutton': 39.2, 'prawn': 18.0, 'crab': 15.5}
# 
#     @staticmethod
#     def calculate_impact(food_type, weight_kg):
#         # Default to a safe fallback key if specific key missing
#         base_type = food_type.lower().split('_')[0] # handle 'Apple_Fresh'
#         factor = CarbonFootprintCalculator.CO2_MAP.get(base_type, 0.5) # Default 0.5 for Fruit/Veg
# 
#         co2 = factor * weight_kg
#         miles = co2 / 0.404
#         return co2, miles
# 
#     @staticmethod
#     def generate_impact_text(food_type, co2_kg):
#         """Generates contextual explanation for carbon footprint"""
#         phones = int(co2_kg / 0.015) # Approx co2 to charge phone
# 
#         severity = ""
#         if food_type.lower() in ['beef', 'mutton']:
#             severity = "This is a **High-Impact** protein source."
#         elif food_type.lower() in ['pork', 'prawn', 'crab', 'chicken', 'fish']:
#             severity = "This is a **Medium-Impact** food source."
#         else:
#             severity = "While **Lower-Impact**, wasting this still affects the environment."
# 
#         return f"""
#         **Environmental Context:**
#         Wasting this amount of **{food_type}** generates **{co2_kg:.2f} kg of CO‚ÇÇe**.
#         To put this in perspective, that is equivalent to the energy required to charge a smartphone **{phones} times**.
#         <br><br>
#         {severity} **SmartShelf AI** helps you consume this before expiry to prevent this unnecessary climate impact.
#         """
# 
# class XAITextGenerator:
#     """Generates text explanations for AI Decisions"""
# 
#     @staticmethod
#     def explain_gradcam(food_name, status, focus_region):
#         """Generates text for Heatmaps"""
#         if 'spoil' in status.lower() or 'rot' in status.lower():
#             reason = "discoloration, surface texture changes, or microbial growth patterns"
#         else:
#             reason = "typical freshness markers like consistent color and firm texture"
# 
#         return f"""
#         **Visual Attention Analysis:**
#         The AI focused heavily on the **{focus_region}** of the image.
#         For **{food_name}**, high activation in this area suggests the model detected {reason}
#         that strongly correlate with the **{status}** classification.
#         """
# 
#     @staticmethod
#     def explain_lime(food_name, status):
#         """Generates text for LIME Boundaries"""
#         return f"""
#         **Feature Boundary Analysis:**
#         The yellow outlines mark the 'Super-Pixels' that positively influenced the decision.
#         The AI isolated these specific segments as the most critical visual evidence for classifying this
#         **{food_name}** as **{status}**. If these regions contain spots or slime, the prediction is highly reliable.
#         """

# Commented out IPython magic to ensure Python compatibility.
# %%writefile /content/drive/MyDrive/Shelf_Life_Project/final_src/app.py
# import streamlit as st
# import tensorflow as tf
# import torch
# import torch.nn as nn
# import timm
# from torchvision import transforms
# import numpy as np
# import pandas as pd
# from PIL import Image
# import plotly.graph_objects as go
# import cv2
# import os
# import sys
# import base64
# from datetime import datetime, timedelta
# from tensorflow.keras import layers, models
# 
# # --- SETUP PATHS ---
# sys.path.append('/content/drive/MyDrive/Shelf_Life_Project/source')
# MODEL_DIR = '/content/drive/MyDrive/Shelf_Life_Project/final_src'
# 
# try:
#     from xai_utils import ExplainableAI
#     import novelty_features as nf
# except ImportError as e:
#     st.error(f"Setup Error: {e}. Ensure xai_utils.py and novelty_features.py are in 'source' folder.")
#     st.stop()
# 
# # --- PAGE CONFIG ---
# st.set_page_config(page_title="SmartShelf AI", page_icon="üß¨", layout="wide", initial_sidebar_state="collapsed")
# 
# # ==========================================
# # 1. DEFINE PYTORCH MODEL ARCHITECTURE (VEG)
# # ==========================================
# class MultiHeadEfficientNet(nn.Module):
#     def __init__(self, num_veg_classes):
#         super().__init__()
#         self.backbone = timm.create_model("efficientnet_b0", pretrained=False, num_classes=0)
#         feat = self.backbone.num_features
#         self.drop = nn.Dropout(0.4)
#         self.veg_head = nn.Linear(feat, num_veg_classes)
#         self.fresh_head = nn.Linear(feat, 3)
#         self.reg_head = nn.Linear(feat, 1)
# 
#     def forward(self, x):
#         f = self.drop(self.backbone(x))
#         return self.veg_head(f), self.fresh_head(f), self.reg_head(f).squeeze(1)
# 
# # ==========================================
# # 2. DEFINE KERAS MODEL ARCHITECTURE (BAKERY)
# # ==========================================
# def build_msff_regressor(input_shape=(224,224,3)):
#     inp = layers.Input(shape=input_shape)
#     b1 = layers.Conv2D(32, 3, padding="same", activation="relu")(inp)
#     b1 = layers.MaxPooling2D(2)(b1)
#     b2 = layers.Conv2D(32, 5, padding="same", activation="relu")(inp)
#     b2 = layers.MaxPooling2D(2)(b2)
#     b3 = layers.Conv2D(32, 7, padding="same", activation="relu")(inp)
#     b3 = layers.MaxPooling2D(2)(b3)
#     x = layers.Concatenate()([b1, b2, b3])
#     x = layers.Conv2D(64, 3, padding="same", activation="relu")(x)
#     x = layers.MaxPooling2D(2)(x)
#     x = layers.Conv2D(128, 3, padding="same", activation="relu", name="msff_last_conv")(x)
#     x = layers.MaxPooling2D(2)(x)
#     x = layers.Flatten()(x)
#     x = layers.Dense(256, activation="relu", name="feature_dense")(x)
#     x = layers.Dropout(0.5)(x)
#     out = layers.Dense(1, activation="linear", name="days_remaining")(x)
#     model = models.Model(inp, out, name="MSFF_Regression")
#     return model
# 
# # --- CUSTOM CSS FOR DASHBOARD ---
# st.markdown("""
# <style>
#     /* Global Font */
#     @import url('https://fonts.googleapis.com/css2?family=Outfit:wght@300;500;700&display=swap');
#     html, body, [class*="css"] { font-family: 'Outfit', sans-serif; }
# 
#     /* Header Styles */
#     .dashboard-header {
#         text-align: center;
#         padding: 60px 20px;
#         background: linear-gradient(120deg, #1E3A5F 0%, #2b5876 100%);
#         border-radius: 25px;
#         color: white;
#         margin-bottom: 50px;
#         box-shadow: 0 10px 30px rgba(30, 58, 95, 0.2);
#     }
#     .main-title { font-size: 4rem; font-weight: 800; margin: 0; color: white; letter-spacing: -1px; }
#     .sub-title { font-size: 1.4rem; color: #aab8c2; margin-top: 10px; font-weight: 300; }
# 
#     /* Modern Image Cards */
#     .category-card {
#         background-color: white;
#         border-radius: 20px;
#         box-shadow: 0 15px 35px rgba(0,0,0,0.08);
#         text-align: center;
#         transition: all 0.4s cubic-bezier(0.175, 0.885, 0.32, 1.275);
#         border: 1px solid #f0f2f5;
#         height: 100%;
#         overflow: hidden;
#         position: relative;
#     }
#     .category-card:hover {
#         transform: translateY(-10px);
#         box-shadow: 0 20px 50px rgba(0,0,0,0.12);
#         border-color: #667eea;
#     }
# 
#     /* Image Styling */
#     .card-img-container {
#         height: 180px;
#         overflow: hidden;
#         position: relative;
#     }
#     .card-img {
#         width: 100%;
#         height: 100%;
#         object-fit: cover;
#         transition: transform 0.5s ease;
#     }
#     .category-card:hover .card-img {
#         transform: scale(1.1);
#     }
# 
#     /* Text Styling */
#     .card-content { padding: 20px; }
#     .card-title { font-size: 1.5rem; font-weight: 700; color: #1E3A5F; margin-bottom: 5px; }
#     .card-desc { font-size: 0.9rem; color: #8898aa; margin-bottom: 20px; line-height: 1.5; }
# 
#     /* Button Styling */
#     div.stButton > button {
#         width: 100%;
#         border-radius: 12px;
#         padding: 0.75rem 1rem;
#         font-weight: 600;
#         border: none;
#         background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);
#         color: white;
#         transition: opacity 0.3s;
#     }
#     div.stButton > button:hover {
#         opacity: 0.9;
#         box-shadow: 0 4px 12px rgba(102, 126, 234, 0.4);
#     }
# 
#     /* Analysis Page Specifics */
#     .metric-container { background: white; padding: 25px; border-radius: 15px; box-shadow: 0 4px 20px rgba(0,0,0,0.05); text-align: center; border: 1px solid #eee; }
#     .xai-box { background-color: #f8f9fa; padding: 20px; border-radius: 12px; border-left: 5px solid #667eea; font-size: 1rem; color: #495057; line-height: 1.6; }
# </style>
# """, unsafe_allow_html=True)
# 
# # --- STATE MANAGEMENT ---
# if 'page' not in st.session_state:
#     st.session_state['page'] = 'home'
# 
# def navigate_to(page):
#     st.session_state['page'] = page
#     st.rerun()
# 
# # --- LOAD RESOURCES ---
# @st.cache_resource
# def load_resources():
#     resources = {}
# 
#     # 1. MEAT MODEL (TensorFlow)
#     tf_path = os.path.join(MODEL_DIR, "final_smartshelf_model.keras")
#     if os.path.exists(tf_path):
#         resources['meat_model'] = tf.keras.models.load_model(tf_path)
#         resources['meat_cat'] = np.load(os.path.join(MODEL_DIR, "classes_category.npy"), allow_pickle=True)
#         resources['meat_stat'] = np.load(os.path.join(MODEL_DIR, "classes_status.npy"), allow_pickle=True)
#         resources['meat_xai'] = ExplainableAI(resources['meat_model'], resources['meat_cat'], backend='tensorflow')
# 
#     # 2. VEG MODEL (PyTorch)
#     pth_path = os.path.join(MODEL_DIR, "best_multitask_model3 (1).pth")
#     if os.path.exists(pth_path):
#         try:
#             checkpoint = torch.load(pth_path, map_location=torch.device('cpu'))
#             veg_map = checkpoint.get("veg_map")
#             if not veg_map:
#                 veg_classes = ['Carrot', 'Cucumber', 'Tomato', 'Potato', 'Brinjal', 'Capsicum']
#             else:
#                 veg_classes = [k for k, v in sorted(veg_map.items(), key=lambda item: item[1])]
# 
#             veg_model = MultiHeadEfficientNet(num_veg_classes=len(veg_classes))
#             veg_model.load_state_dict(checkpoint["model"])
#             veg_model.eval()
# 
#             resources['veg_model'] = veg_model
#             resources['veg_classes'] = veg_classes
#             resources['veg_fresh_classes'] = ['Fresh', 'Mid', 'Spoiled']
#             resources['veg_xai'] = ExplainableAI(veg_model, veg_classes, backend='pytorch')
#         except Exception as e:
#             print(f"Failed to load PyTorch model: {e}")
# 
#     # 3. BAKERY MODEL (TensorFlow/Keras)
#     bakery_path = os.path.join(MODEL_DIR, "msff_bread_model.keras")
#     if os.path.exists(bakery_path):
#         try:
#             try:
#                 resources['bakery_model'] = tf.keras.models.load_model(bakery_path)
#             except:
#                 model = build_msff_regressor()
#                 model.load_weights(bakery_path)
#                 resources['bakery_model'] = model
# 
#             resources['bakery_classes'] = ['Bread']
#             resources['bakery_xai'] = ExplainableAI(resources['bakery_model'], ['Bread'], backend='tensorflow')
#         except Exception as e:
#              print(f"Failed to load Bakery model: {e}")
# 
#     # 4. FRUIT MODEL (TensorFlow/Keras)
#     fruit_path = "/content/drive/MyDrive/Shelf_Life_Project/final_src/fruit_shelf_life_model.h5"
#     if os.path.exists(fruit_path):
#         try:
#              # compile=False helps avoid issues with custom metrics/losses from older Keras versions
#              resources['fruit_model'] = tf.keras.models.load_model(fruit_path, compile=False)
#              resources['fruit_classes'] = ["Apple", "Banana", "Guava", "Orange", "Pomegranate", "Strawberry"]
#              resources['fruit_xai'] = ExplainableAI(resources['fruit_model'], resources['fruit_classes'], backend='tensorflow')
#              print("SUCCESS: Fruit model loaded!")
#         except Exception as e:
#              print(f"Failed to load Fruit model: {e}")
#     else:
#          print(f"CRITICAL ERROR: Fruit model file missing at {fruit_path}")
# 
#     return resources
# 
# res = load_resources()
# 
# # --- HELPER FUNCTIONS ---
# def get_img_as_base64(file_path):
#     """Converts local image to base64 string for HTML embedding"""
#     if not os.path.exists(file_path):
#         return ""
#     with open(file_path, "rb") as f:
#         data = f.read()
#     return base64.b64encode(data).decode()
# 
# def preprocess_image(image):
#     img = image.resize((380, 380))
#     img_array = np.array(img)
#     if img_array.ndim == 2: img_array = cv2.cvtColor(img_array, cv2.COLOR_GRAY2RGB)
#     elif img_array.shape[2] == 4: img_array = img_array[..., :3]
#     img_pre = tf.keras.applications.efficientnet.preprocess_input(img_array.astype(np.float32))
#     return img_pre, img_array
# 
# def preprocess_torch(image):
#     if image.mode != 'RGB': image = image.convert('RGB')
# 
#     # 1. Create a LARGE copy for Display/XAI (this fixes the blur)
#     img_display = image.resize((380, 380))
# 
#     # 2. Create a SMALL copy for the Model (must match training size)
#     img_model = image.resize((96, 96))
# 
#     # 3. Process only the small one for the AI
#     arr = np.array(img_model).astype(np.float32) / 255.0
#     mean = np.array([0.485, 0.456, 0.406])
#     std = np.array([0.229, 0.224, 0.225])
#     arr = (arr - mean) / std
# 
#     # 4. Return the PROCESSED small image for the model,
#     #    but the HIGH-RES display image for XAI
#     return arr, np.array(img_display)
# 
# def preprocess_bakery(image):
#     if image.mode != 'RGB': image = image.convert('RGB')
#     img = image.resize((224, 224))
#     arr = np.array(img).astype(np.float32) / 255.0
#     return np.expand_dims(arr, axis=0), np.array(img)
# 
# def preprocess_fruit(image):
#     # FIXED: Convert RGB to BGR for Fruit Model
#     if image.mode != 'RGB': image = image.convert('RGB')
#     img_arr = np.array(image)
#     if img_arr.ndim == 2: img_arr = cv2.cvtColor(img_arr, cv2.COLOR_GRAY2RGB)
#     if img_arr.shape[-1] == 4: img_arr = img_arr[..., :3]
# 
#     img_bgr = cv2.cvtColor(img_arr, cv2.COLOR_RGB2BGR)
#     img_resized = cv2.resize(img_bgr, (160, 160))
#     arr = img_resized.astype(np.float32) / 255.0
#     return np.expand_dims(arr, axis=0), img_arr
# 
# def get_grade(score, status):
#     if status == 'spoiled' or 'Rotten' in status or status == 'spoiled': return "F", "#ef476f"
#     if score > 0.8: return "A+", "#06d6a0"
#     if score > 0.6: return "A", "#26de81"
#     if score > 0.4: return "B", "#ffd166"
#     return "C", "#ff9f43"
# 
# def get_bakery_freshness(days_remaining):
#     if days_remaining >= 5: return "Fresh", "#06d6a0"
#     elif days_remaining >= 3: return "Mild", "#ffd166"
#     elif days_remaining >= 1: return "Okay", "#ff9f43"
#     else: return "Spoiled", "#ef476f"
# 
# # --- PAGE: HOME DASHBOARD ---
# def render_home():
#     st.markdown("""
#         <div class="dashboard-header">
#             <h1 class="main-title">SmartShelf AI</h1>
#             <p class="sub-title">The Next Generation of Food Quality Intelligence</p>
#         </div>
#     """, unsafe_allow_html=True)
# 
#     col1, col2, col3, col4 = st.columns(4)
# 
#     path_meat = "/content/drive/MyDrive/Shelf_Life_Project/final_src/meat.png"
#     path_fruit = "/content/drive/MyDrive/Shelf_Life_Project/final_src/fruits.png"
#     path_veg = "/content/drive/MyDrive/Shelf_Life_Project/final_src/vegetables.png"
#     path_bakery = "/content/drive/MyDrive/Shelf_Life_Project/final_src/bakery.png"
# 
#     b64_meat = get_img_as_base64(path_meat)
#     b64_fruit = get_img_as_base64(path_fruit)
#     b64_veg = get_img_as_base64(path_veg)
#     b64_bakery = get_img_as_base64(path_bakery)
# 
#     placeholder = "https://via.placeholder.com/400x300?text=Image+Not+Found"
# 
#     src_meat = f"data:image/png;base64,{b64_meat}" if b64_meat else placeholder
#     src_fruit = f"data:image/png;base64,{b64_fruit}" if b64_fruit else placeholder
#     src_veg = f"data:image/png;base64,{b64_veg}" if b64_veg else placeholder
#     src_bakery = f"data:image/png;base64,{b64_bakery}" if b64_bakery else placeholder
# 
#     with col1:
#         st.markdown(f"""
#             <div class="category-card">
#                 <div class="card-img-container"><img src="{src_meat}" class="card-img"></div>
#                 <div class="card-content">
#                     <div class="card-title">Meat & Seafood</div>
#                     <div class="card-desc">Multi-Task Protein Quality Analysis.</div>
#                 </div>
#             </div>
#         """, unsafe_allow_html=True)
#         if st.button("Launch Analysis", key="btn_meat"): navigate_to("meat")
# 
#     with col2:
#         st.markdown(f"""
#             <div class="category-card">
#                 <div class="card-img-container"><img src="{src_fruit}" class="card-img"></div>
#                 <div class="card-content">
#                     <div class="card-title">Fruits</div>
#                     <div class="card-desc">Ripeness and freshness detection.</div>
#                 </div>
#             </div>
#         """, unsafe_allow_html=True)
#         if st.button("Launch Analysis", key="btn_fruit"): navigate_to("fruit")
# 
#     with col3:
#         st.markdown(f"""
#             <div class="category-card">
#                 <div class="card-img-container"><img src="{src_veg}" class="card-img"></div>
#                 <div class="card-content">
#                     <div class="card-title">Vegetables</div>
#                     <div class="card-desc">Quality grading for greens and roots.</div>
#                 </div>
#             </div>
#         """, unsafe_allow_html=True)
#         if st.button("Launch Analysis", key="btn_veg"): navigate_to("veg")
# 
#     with col4:
#         st.markdown(f"""
#             <div class="category-card">
#                 <div class="card-img-container"><img src="{src_bakery}" class="card-img"></div>
#                 <div class="card-content">
#                     <div class="card-title">Bakery</div>
#                     <div class="card-desc">Mold detection for baked goods.</div>
#                 </div>
#             </div>
#         """, unsafe_allow_html=True)
#         if st.button("Launch Analysis", key="btn_bakery"): navigate_to("bakery")
# 
#     st.markdown("---")
#     c1, c2, c3 = st.columns(3)
#     c1.metric("System Status", "Online ‚úÖ")
#     c2.metric("Models Active", "4") # Meat + Veg + Bakery + Fruit
#     c3.metric("Last Update", datetime.now().strftime("%B %d, %Y"))
# 
# # --- PAGE: MEAT MODULE ---
# def render_meat_page():
#     # Header with Back Navigation
#     c_back, c_title = st.columns([1, 8])
#     with c_back:
#         if st.button("‚¨Ö Home"): navigate_to("home")
#     with c_title:
#         st.markdown("## ü•© Meat & Seafood Analysis")
# 
#     if 'meat_model' not in res:
#         st.error("Model not found in `final_src`. Please verify files.")
#         return
# 
#     model = res['meat_model']
#     cat_classes = res['meat_cat']
#     stat_classes = res['meat_stat']
#     xai = res['meat_xai']
# 
#     c_side, c_main = st.columns([1, 2])
# 
#     with c_side:
#         st.markdown("### Input Source")
#         mode = st.radio("Select:", ["Upload Image", "Live Camera"], label_visibility="collapsed")
# 
#         img_file = None
#         if mode == "Upload Image":
#             img_file = st.file_uploader("Upload", type=['jpg','png','jpeg'])
#         else:
#             img_file = st.camera_input("Capture")
# 
#         if img_file:
#             image_pil = Image.open(img_file)
#             st.image(image_pil, caption="Sample", use_container_width=True)
#             analyze = st.button("üî¨ Run Diagnostics", type="primary", use_container_width=True)
#         else:
#             analyze = False
# 
#     with c_main:
#         if img_file and analyze:
#             with st.spinner("üß† AI is analyzing texture patterns..."):
#                 img_pre, img_orig = preprocess_image(image_pil)
#                 img_batch = np.expand_dims(img_pre, axis=0)
#                 preds = model.predict(img_batch, verbose=0)
# 
#                 cat_idx = np.argmax(preds[0])
#                 food_name = cat_classes[cat_idx]
#                 conf = np.max(preds[0])
#                 stat_idx = np.argmax(preds[1])
#                 status = stat_classes[stat_idx]
#                 score = float(preds[2][0][0])
# 
#                 max_life = 5.0
#                 if status == 'spoiled': days_left = 0.0; score = 0.1
#                 else: days_left = score * max_life
# 
#                 expiry_str = (datetime.now() + timedelta(days=days_left)).strftime("%b %d")
#                 grade, color = get_grade(score, status)
# 
#                 st.session_state['meat_res'] = {
#                     'food': food_name, 'status': status, 'days': days_left,
#                     'score': score, 'grade': grade, 'color': color, 'expiry': expiry_str,
#                     'img_pre': img_pre, 'cat_idx': cat_idx, 'stat_idx': stat_idx,
#                     'img_orig': img_orig # Save original for XAI visualization
#                 }
#                 if 'xai_data' in st.session_state: del st.session_state['xai_data']
# 
#         if 'meat_res' in st.session_state:
#             result = st.session_state['meat_res']
# 
#             m1, m2, m3 = st.columns(3)
#             m1.markdown(f"<div class='metric-container'><h4>Detected</h4><h2 style='color:#1E3A5F'>{result['food'].upper()}</h2></div>", unsafe_allow_html=True)
#             m2.markdown(f"<div class='metric-container' style='border-bottom: 5px solid {result['color']}'><h4>Condition</h4><h2 style='color:{result['color']}'>{result['status'].upper()}</h2></div>", unsafe_allow_html=True)
#             m3.markdown(f"<div class='metric-container'><h4>Shelf Life</h4><h2>{result['days']:.1f} Days</h2><small>Exp: {result['expiry']}</small></div>", unsafe_allow_html=True)
# 
#             st.markdown("---")
# 
#             t1, t2, t3 = st.tabs(["üìâ Projections", "üß† Explainable AI", "üåç Sustainability"])
# 
#             with t1:
#                 col_a, col_b = st.columns([2,1])
#                 with col_a: st.plotly_chart(nf.QualityDecayPredictor.predict_decay_curve(result['score'], result['days']), use_container_width=True)
#                 with col_b:
#                     st.write("**AI Storage Advice**")
#                     for i, title, desc in nf.SmartStorageAdvisor.generate_recommendations(result['food'], result['score']*100, result['days']):
#                         st.info(f"{i} **{title}**: {desc}")
# 
#             with t2:
#                 st.info("Visualizing decision logic with Grad-CAM and LIME.")
# 
#                 if 'xai_data' not in st.session_state:
#                     if st.button("Generate Deep Explanation (~10s)", use_container_width=True):
#                         with st.spinner("Calculating Gradients & Superpixels..."):
#                             hm_food, hm_fresh = xai.grad_cam(result['img_pre'])
#                             focus = xai.analyze_heatmap_focus(hm_fresh)
#                             lime = xai.lime_explanation(result['img_orig'], target_head=1, num_samples=50)
#                             st.session_state['xai_data'] = {'hm': hm_fresh, 'focus': focus, 'lime': lime}
#                             st.rerun()
#                 else:
#                     xd = st.session_state['xai_data']
#                     v_grad = xai.visualize_grad_cam(result['img_pre'], xd['hm'], alpha=0.4)
#                     v_lime = xai.visualize_lime(xd['lime'], result['stat_idx'], num_features=5)
# 
#                     r1, r2 = st.columns(2)
#                     with r1:
#                         # FIX: Added clamp=True to prevent range error
#                         st.image(v_grad, caption="Grad-CAM (Attention)", use_container_width=True, clamp=True)
#                         grad_text = nf.XAITextGenerator.explain_gradcam(result['food'], result['status'], xd['focus'])
#                         st.markdown(f"<div class='xai-box'>{grad_text}</div>", unsafe_allow_html=True)
#                     with r2:
#                         # FIX: Added clamp=True to prevent range error
#                         st.image(v_lime, caption="LIME Features", use_container_width=True, clamp=True)
#                         lime_text = nf.XAITextGenerator.explain_lime(result['food'], result['status'])
#                         st.markdown(f"<div class='xai-box'>{lime_text}</div>", unsafe_allow_html=True)
# 
#                     if st.button("Reset Explanation"): del st.session_state['xai_data']; st.rerun()
# 
#             with t3:
#                 co2, miles = nf.CarbonFootprintCalculator.calculate_impact(result['food'], 0.5)
#                 st.metric("Estimated Footprint", f"{co2:.2f} kg CO2e", delta=f"{miles:.1f} car miles")
#                 st.markdown(f"<div class='xai-box' style='border-left-color: #28a745'>{nf.CarbonFootprintCalculator.generate_impact_text(result['food'], co2)}</div>", unsafe_allow_html=True)
# 
# # --- PAGE: VEGETABLE MODULE ---
# def render_veg_page():
#     if st.button("‚Üê Back to Dashboard"):
#         navigate_to("home")
# 
#     st.markdown("## ü•¶ Vegetable Quality Analysis")
# 
#     if 'veg_model' not in res:
#         st.error("Veg model missing or failed to load. Ensure 'best_multitask_model3 (1).pth' is in `final_src`.")
#         return
# 
#     model = res['veg_model']
#     veg_classes = res['veg_classes']
#     fresh_classes = res['veg_fresh_classes']
#     xai = res['veg_xai']
# 
#     c_side, c_main = st.columns([1, 2])
# 
#     with c_side:
#         st.markdown("### Input Source")
#         mode = st.radio("Select:", ["Upload Image", "Live Camera"], label_visibility="collapsed", key="veg_mode")
# 
#         img_file = None
#         if mode == "Upload Image":
#             img_file = st.file_uploader("Upload", type=['jpg','png','jpeg'], key="veg_upload")
#         else:
#             img_file = st.camera_input("Capture", key="veg_cam")
# 
#         if img_file:
#             image_pil = Image.open(img_file)
#             st.image(image_pil, caption="Sample", use_container_width=True)
#             analyze = st.button("üî¨ Analyze Veg", type="primary", use_container_width=True)
#         else:
#             analyze = False
# 
#     with c_main:
#         if img_file and analyze:
#             with st.spinner("ü•¶ Analyzing Vegetable Freshness..."):
#                 # Use CORRECT Preprocessing for Veg (96x96)
#                 img_pre, img_orig = preprocess_torch(image_pil)
#                 tensor = torch.from_numpy(img_pre).permute(2, 0, 1).unsqueeze(0).float()
# 
#                 with torch.no_grad():
#                     veg_out, fresh_out, reg_out = model(tensor)
#                     veg_probs = torch.softmax(veg_out, dim=1)
#                     veg_idx = torch.argmax(veg_probs).item()
#                     food = veg_classes[veg_idx]
# 
#                     fresh_probs = torch.softmax(fresh_out, dim=1)
#                     fresh_idx = torch.argmax(fresh_probs).item()
#                     status = fresh_classes[fresh_idx]
# 
#                     shelf_life = max(0.0, reg_out.item())
# 
#                 # Logic Fix: Map "Fresh" to High Score
#                 score = min(1.0, shelf_life / 14.0)
#                 if status == 'Fresh' or status == 'Mid':
#                     score = max(score, 0.45) # Ensure it passes 0.40 threshold for "Do Not Consume"
#                 if status == 'Rotten' or status == 'spoiled':
#                     score = 0.1
# 
#                 grade, color = get_grade(score, status)
#                 expiry_str = (datetime.now() + timedelta(days=float(shelf_life))).strftime("%b %d")
# 
#                 st.session_state['veg_res'] = {
#                     'food': food, 'status': status, 'days': shelf_life,
#                     'score': score, 'grade': grade, 'color': color, 'expiry': expiry_str,
#                     'img_pre': img_pre, 'stat_idx': fresh_idx, # Store index for XAI
#                     'img_orig': img_orig # Store original for LIME
#                 }
#                 if 'veg_xai_data' in st.session_state: del st.session_state['veg_xai_data']
# 
#         if 'veg_res' in st.session_state:
#             result = st.session_state['veg_res']
# 
#             m1, m2, m3 = st.columns(3)
#             m1.markdown(f"<div class='metric-container'><h4>Detected</h4><h2 style='color:#1E3A5F'>{result['food']}</h2></div>", unsafe_allow_html=True)
#             m2.markdown(f"<div class='metric-container' style='border-bottom: 5px solid {result['color']}'><h4>Condition</h4><h2 style='color:{result['color']}'>{result['status']}</h2></div>", unsafe_allow_html=True)
#             m3.markdown(f"<div class='metric-container'><h4>Shelf Life</h4><h2>{result['days']:.1f} Days</h2><small>Exp: {result['expiry']}</small></div>", unsafe_allow_html=True)
# 
#             st.markdown("---")
# 
#             t1, t2, t3 = st.tabs(["üìâ Projections", "üß† Explainable AI", "üåç Sustainability"])
# 
#             with t1:
#                 col_a, col_b = st.columns([2,1])
#                 with col_a: st.plotly_chart(nf.QualityDecayPredictor.predict_decay_curve(result['score'], result['days']), use_container_width=True)
#                 with col_b:
#                     st.write("**AI Storage Advice**")
#                     for i, title, desc in nf.SmartStorageAdvisor.generate_recommendations(result['food'], result['score']*100, result['days']):
#                         st.info(f"{i} **{title}**: {desc}")
# 
#             with t2:
#                 st.info("Visualizing decision logic with Grad-CAM and LIME.")
#                 if 'veg_xai_data' not in st.session_state:
#                     if st.button("Generate Explanation (~10s)"):
#                         with st.spinner("Computing..."):
#                             hm, _ = xai.grad_cam(result['img_pre'])
#                             focus = xai.analyze_heatmap_focus(hm)
#                             # Target Freshness Head (Index 1) for LIME
#                             lime = xai.lime_explanation(result['img_orig'], target_head=1, num_samples=50) # Use img_orig
#                             st.session_state['veg_xai_data'] = {'hm': hm, 'focus': focus, 'lime': lime}
#                             st.rerun()
#                 else:
#                     xd = st.session_state['veg_xai_data']
#                     v_grad = xai.visualize_grad_cam(result['img_pre'], xd['hm'], alpha=0.4)
#                     v_lime = xai.visualize_lime(xd['lime'], result['stat_idx'], num_features=5)
#                     r1, r2 = st.columns(2)
#                     with r1:
#                         st.image(v_grad, caption="Grad-CAM Attention", use_container_width=True, clamp=True)
#                         grad_text = nf.XAITextGenerator.explain_gradcam(result['food'], result['status'], xd['focus'])
#                         st.markdown(f"<div class='xai-box'>{grad_text}</div>", unsafe_allow_html=True)
#                     with r2:
#                         st.image(v_lime, caption="LIME Features", use_container_width=True, clamp=True)
#                         lime_text = nf.XAITextGenerator.explain_lime(result['food'], result['status'])
#                         st.markdown(f"<div class='xai-box'>{lime_text}</div>", unsafe_allow_html=True)
#                     if st.button("Reset Explanation"): del st.session_state['veg_xai_data']; st.rerun()
# 
#             with t3:
#                  co2, miles = nf.CarbonFootprintCalculator.calculate_impact(result['food'], 0.5)
#                  st.metric("Estimated Footprint", f"~0.5 kg CO2e", delta="Low Impact")
#                  st.markdown(f"<div class='xai-box' style='border-left-color: #28a745'>{nf.CarbonFootprintCalculator.generate_impact_text(result['food'], co2)}</div>", unsafe_allow_html=True)
# 
# # --- PAGE: BAKERY MODULE ---
# def render_bakery_page():
#     if st.button("‚Üê Back to Dashboard"):
#         navigate_to("home")
# 
#     st.markdown("## üçû Bakery Quality Analysis")
# 
#     if 'bakery_model' not in res:
#         st.error("Bakery model missing or failed to load. Ensure 'msff_bread_model.keras' is in `final_src`.")
#         return
# 
#     model = res['bakery_model']
#     xai = res['bakery_xai']
# 
#     c_side, c_main = st.columns([1, 2])
# 
#     with c_side:
#         st.markdown("### Input Source")
#         mode = st.radio("Select:", ["Upload Image", "Live Camera"], label_visibility="collapsed", key="bake_mode")
# 
#         img_file = None
#         if mode == "Upload Image":
#             img_file = st.file_uploader("Upload", type=['jpg','png','jpeg'], key="bake_upload")
#         else:
#             img_file = st.camera_input("Capture", key="bake_cam")
# 
#         if img_file:
#             image_pil = Image.open(img_file)
#             st.image(image_pil, caption="Sample", use_container_width=True)
#             analyze = st.button("üî¨ Analyze Bakery", type="primary", use_container_width=True)
#         else:
#             analyze = False
# 
#     with c_main:
#         if img_file and analyze:
#             with st.spinner("üçû Analyzing Bakery Freshness..."):
#                 img_batch, img_orig = preprocess_bakery(image_pil)
# 
#                 # Inference
#                 days_reg = model.predict(img_batch, verbose=0)[0][0]
# 
#                 # Logic
#                 shelf_life = max(0.0, float(days_reg))
#                 status, color = get_bakery_freshness(shelf_life)
#                 food = "Bread"
# 
#                 # Logic Fix
#                 score = min(1.0, shelf_life / 14.0)
#                 if status == 'Fresh' or status == 'Medium': score = max(score, 0.45)
# 
#                 grade, _ = get_grade(score, status)
#                 expiry_str = (datetime.now() + timedelta(days=float(shelf_life))).strftime("%b %d")
# 
#                 st.session_state['bake_res'] = {
#                     'food': food, 'status': status, 'days': shelf_life,
#                     'score': score, 'grade': grade, 'color': color, 'expiry': expiry_str,
#                     'img_pre': img_batch[0], # Pass 3D array for XAI
#                     'img_orig': img_orig,
#                     'stat_idx': 0 # Dummy index for regression XAI
#                 }
#                 if 'bake_xai_data' in st.session_state: del st.session_state['bake_xai_data']
# 
#         if 'bake_res' in st.session_state:
#             result = st.session_state['bake_res']
# 
#             m1, m2, m3 = st.columns(3)
#             m1.markdown(f"<div class='metric-container'><h4>Detected</h4><h2 style='color:#1E3A5F'>{result['food']}</h2></div>", unsafe_allow_html=True)
#             m2.markdown(f"<div class='metric-container' style='border-bottom: 5px solid {result['color']}'><h4>Condition</h4><h2 style='color:{result['color']}'>{result['status']}</h2></div>", unsafe_allow_html=True)
#             m3.markdown(f"<div class='metric-container'><h4>Shelf Life</h4><h2>{result['days']:.1f} Days</h2><small>Exp: {result['expiry']}</small></div>", unsafe_allow_html=True)
# 
#             st.markdown("---")
# 
#             # UNIFIED TABS for Bakery
#             t1, t2, t3 = st.tabs(["üìâ Projections", "üß† Explainable AI", "üåç Sustainability"])
# 
#             with t1:
#                 col_a, col_b = st.columns([2,1])
#                 with col_a: st.plotly_chart(nf.QualityDecayPredictor.predict_decay_curve(result['score'], result['days']), use_container_width=True)
#                 with col_b:
#                     st.write("**AI Storage Advice**")
#                     for i, title, desc in nf.SmartStorageAdvisor.generate_recommendations(result['food'], result['score']*100, result['days']):
#                         st.info(f"{i} **{title}**: {desc}")
# 
#             with t2:
#                 st.info("Visualizing decision logic with Grad-CAM and LIME.")
#                 if 'bake_xai_data' not in st.session_state:
#                     if st.button("Generate Explanation (~10s)"):
#                         with st.spinner("Computing..."):
#                             hm, _ = xai.grad_cam(result['img_pre'])
#                             focus = xai.analyze_heatmap_focus(hm)
#                             lime = xai.lime_explanation(result['img_orig'], num_samples=50) # Use img_orig
#                             st.session_state['bake_xai_data'] = {'hm': hm, 'focus': focus, 'lime': lime}
#                             st.rerun()
#                 else:
#                     xd = st.session_state['bake_xai_data']
#                     v_grad = xai.visualize_grad_cam(result['img_pre'], xd['hm'], alpha=0.4)
#                     v_lime = xai.visualize_lime(xd['lime'], result['stat_idx'], num_features=5)
#                     r1, r2 = st.columns(2)
#                     with r1:
#                         st.image(v_grad, caption="Grad-CAM (Attention)", use_container_width=True, clamp=True)
#                         grad_text = nf.XAITextGenerator.explain_gradcam(result['food'], result['status'], xd['focus'])
#                         st.markdown(f"<div class='xai-box'>{grad_text}</div>", unsafe_allow_html=True)
#                     with r2:
#                         st.image(v_lime, caption="LIME Features", use_container_width=True, clamp=True)
#                         lime_text = nf.XAITextGenerator.explain_lime(result['food'], result['status'])
#                         st.markdown(f"<div class='xai-box'>{lime_text}</div>", unsafe_allow_html=True)
#                     if st.button("Reset Explanation"): del st.session_state['bake_xai_data']; st.rerun()
# 
#             with t3:
#                  co2, miles = nf.CarbonFootprintCalculator.calculate_impact(result['food'], 0.5)
#                  st.metric("Estimated Footprint", f"~0.8 kg CO2e", delta="Medium Impact")
#                  st.markdown(f"<div class='xai-box' style='border-left-color: #28a745'>{nf.CarbonFootprintCalculator.generate_impact_text(result['food'], co2)}</div>", unsafe_allow_html=True)
# 
# # --- PAGE: FRUIT MODULE ---
# def render_fruit_page():
#     if st.button("‚Üê Back to Dashboard"):
#         navigate_to("home")
# 
#     st.markdown("## üçé Fruit Quality Analysis")
# 
#     if 'fruit_model' not in res:
#         st.error("Fruit model missing or failed to load. Ensure 'fruit_shelf_life_model.h5' is in `final_src`.")
#         return
# 
#     model = res['fruit_model']
#     fruit_classes = res['fruit_classes']
#     xai = res['fruit_xai']
# 
#     c_side, c_main = st.columns([1, 2])
# 
#     with c_side:
#         st.markdown("### Input Source")
#         mode = st.radio("Select:", ["Upload Image", "Live Camera"], label_visibility="collapsed", key="fruit_mode")
# 
#         img_file = None
#         if mode == "Upload Image":
#             img_file = st.file_uploader("Upload", type=['jpg','png','jpeg'], key="fruit_upload")
#         else:
#             img_file = st.camera_input("Capture", key="fruit_cam")
# 
#         if img_file:
#             image_pil = Image.open(img_file)
#             st.image(image_pil, caption="Sample", use_container_width=True)
#             analyze = st.button("üî¨ Analyze Fruit", type="primary", use_container_width=True)
#         else:
#             analyze = False
# 
#     with c_main:
#         if img_file and analyze:
#             with st.spinner("üçé Analyzing Fruit Freshness..."):
#                 # Preprocess for Fruit Model (160x160)
#                 img_batch, img_orig = preprocess_fruit(image_pil)
# 
#                 # Inference
#                 fruit_pred, fresh_pred, days_pred = model.predict(img_batch, verbose=0)
# 
#                 # Decode
#                 fruit_idx = np.argmax(fruit_pred)
#                 food = fruit_classes[fruit_idx]
# 
#                 # Freshness Logic (from snippet: < 0.5 is fresh)
#                 status = "Fresh" if fresh_pred[0][0] < 0.5 else "Spoiled"
#                 shelf_life = max(0.0, float(days_pred[0][0]))
# 
#                 # Color logic
#                 color = "#06d6a0" if status == "Fresh" else "#ef476f"
#                 expiry_str = (datetime.now() + timedelta(days=float(shelf_life))).strftime("%b %d")
# 
#                 # Fake score for consistency
#                 score = 0.9 if status == "Fresh" else 0.1
#                 grade, _ = get_grade(score, status)
# 
#                 stat_idx = 0 if status == "Fresh" else 1
# 
#                 st.session_state['fruit_res'] = {
#                     'food': food, 'status': status, 'days': shelf_life,
#                     'score': score, 'color': color, 'expiry': expiry_str,
#                     'img_pre': img_batch[0], # Pass for XAI
#                     'img_orig': img_orig,
#                     'stat_idx': stat_idx,
#                     'grade': grade
#                 }
#                 if 'fruit_xai_data' in st.session_state: del st.session_state['fruit_xai_data']
# 
#         if 'fruit_res' in st.session_state:
#             result = st.session_state['fruit_res']
# 
#             m1, m2, m3 = st.columns(3)
#             m1.markdown(f"<div class='metric-container'><h4>Detected</h4><h2 style='color:#1E3A5F'>{result['food']}</h2></div>", unsafe_allow_html=True)
#             m2.markdown(f"<div class='metric-container' style='border-bottom: 5px solid {result['color']}'><h4>Condition</h4><h2 style='color:{result['color']}'>{result['status']}</h2></div>", unsafe_allow_html=True)
#             m3.markdown(f"<div class='metric-container'><h4>Shelf Life</h4><h2>{result['days']:.1f} Days</h2><small>Exp: {result['expiry']}</small></div>", unsafe_allow_html=True)
# 
#             st.markdown("---")
# 
#             # UNIFIED TABS for Fruit
#             t1, t2, t3 = st.tabs(["üìâ Projections", "üß† Explainable AI", "üåç Sustainability"])
# 
#             with t1:
#                 col_a, col_b = st.columns([2,1])
#                 with col_a: st.plotly_chart(nf.QualityDecayPredictor.predict_decay_curve(result['score'], result['days']), use_container_width=True)
#                 with col_b:
#                     st.write("**AI Storage Advice**")
#                     for i, title, desc in nf.SmartStorageAdvisor.generate_recommendations(result['food'], result['score']*100, result['days']):
#                         st.info(f"{i} **{title}**: {desc}")
# 
#             with t2:
#                 st.info("Visualizing decision logic with Grad-CAM and LIME.")
#                 if 'fruit_xai_data' not in st.session_state:
#                     if st.button("Generate Explanation (~10s)"):
#                         with st.spinner("Computing..."):
#                             # Target Freshness Head (Index 1) for Fruit model
#                             hm, _ = xai.grad_cam(result['img_pre'])
#                             focus = xai.analyze_heatmap_focus(hm)
#                             lime = xai.lime_explanation(result['img_orig'], target_head=1, num_samples=50) # Use img_orig
#                             st.session_state['fruit_xai_data'] = {'hm': hm, 'focus': focus, 'lime': lime}
#                             st.rerun()
#                 else:
#                     xd = st.session_state['fruit_xai_data']
#                     v_grad = xai.visualize_grad_cam(result['img_pre'], xd['hm'], alpha=0.4)
#                     v_lime = xai.visualize_lime(xd['lime'], result['stat_idx'], num_features=5) # Fixed index
#                     r1, r2 = st.columns(2)
#                     with r1:
#                         st.image(v_grad, caption="Grad-CAM (Attention)", use_container_width=True, clamp=True)
#                         grad_text = nf.XAITextGenerator.explain_gradcam(result['food'], result['status'], xd['focus'])
#                         st.markdown(f"<div class='xai-box'>{grad_text}</div>", unsafe_allow_html=True)
#                     with r2:
#                         st.image(v_lime, caption="LIME Features", use_container_width=True, clamp=True)
#                         lime_text = nf.XAITextGenerator.explain_lime(result['food'], result['status'])
#                         st.markdown(f"<div class='xai-box'>{lime_text}</div>", unsafe_allow_html=True)
#                     if st.button("Reset Explanation"): del st.session_state['fruit_xai_data']; st.rerun()
# 
#             with t3:
#                  co2, miles = nf.CarbonFootprintCalculator.calculate_impact(result['food'], 0.5)
#                  st.metric("Estimated Footprint", f"~0.3 kg CO2e", delta="Low Impact")
#                  st.markdown(f"<div class='xai-box' style='border-left-color: #28a745'>{nf.CarbonFootprintCalculator.generate_impact_text(result['food'], co2)}</div>", unsafe_allow_html=True)
# 
# 
# # --- MAIN ROUTER ---
# if st.session_state['page'] == 'home':
#     render_home()
# elif st.session_state['page'] == 'meat':
#     render_meat_page()
# elif st.session_state['page'] == 'fruit':
#     render_fruit_page()
# elif st.session_state['page'] == 'veg':
#     render_veg_page()
# elif st.session_state['page'] == 'bakery':
#     render_bakery_page()

from pyngrok import ngrok
import os

# --- CONFIGURATION ---
# Path to your final source folder where app.py lives
# Updated to point to 'final_src' as per your project structure
PROJECT_PATH_ABS = "/content/drive/MyDrive/Shelf_Life_Project/final_src"
APP_FILE = os.path.join(PROJECT_PATH_ABS, "app.py")

# --- NGROK SETUP ---
# Terminate any existing ngrok tunnels to avoid conflicts
ngrok.kill()

# Set your auth token
ngrok.set_auth_token("3449PCqNGRpO2QTeKzEeYrvrdgx_2EPLQokeFLQg7mTT1A2wd")

# --- DEPLOY ---
# Open a HTTP tunnel on the default Streamlit port 8501
public_url = ngrok.connect(8501)
print(f"üöÄ Your SmartShelf App is LIVE at: {public_url}")

# Run Streamlit
# We use the absolute path to ensure it finds the correct app.py
# Using os.system allows us to see the logs directly in the cell output
print(f"Starting Streamlit from: {APP_FILE}")
os.system(f"streamlit run '{APP_FILE}' --server.port 8501")

# --- Install Libraries ---
!pip install streamlit pyngrok shap lime -q

!pip install lime plotly streamlit-lottie pyngrok scikit-image

!pip install lime shap plotly streamlit-lottie pyngrok